{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8526810,"sourceType":"datasetVersion","datasetId":5091890}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load DFs after importing pandas\nimport pandas as pd\ntraindf= pd.read_csv(\"/kaggle/input/wids2-dfs/WIDS2_train.csv\")\ntestdf= pd.read_csv (\"/kaggle/input/wids2-dfs/WIDS2_test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T22:59:12.439143Z","iopub.execute_input":"2024-05-31T22:59:12.439514Z","iopub.status.idle":"2024-05-31T22:59:14.424544Z","shell.execute_reply.started":"2024-05-31T22:59:12.439482Z","shell.execute_reply":"2024-05-31T22:59:14.423409Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"num_rows_shape = testdf.shape[0]\nprint(\"Number of rows in DataFrame (using shape):\", num_rows_shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:34:55.590120Z","iopub.execute_input":"2024-05-29T16:34:55.590574Z","iopub.status.idle":"2024-05-29T16:34:55.595829Z","shell.execute_reply.started":"2024-05-29T16:34:55.590534Z","shell.execute_reply":"2024-05-29T16:34:55.594626Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"Number of rows in DataFrame (using shape): 5646\n","output_type":"stream"}]},{"cell_type":"code","source":"traindf.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T15:29:45.586387Z","iopub.execute_input":"2024-05-29T15:29:45.586793Z","iopub.status.idle":"2024-05-29T15:29:45.623200Z","shell.execute_reply.started":"2024-05-29T15:29:45.586762Z","shell.execute_reply":"2024-05-29T15:29:45.622271Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 13173 entries, 0 to 13172\nColumns: 152 entries, patient_id to metastatic_diagnosis_period\ndtypes: float64(137), int64(4), object(11)\nmemory usage: 15.3+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"testdf.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:22:48.129816Z","iopub.execute_input":"2024-05-29T16:22:48.130208Z","iopub.status.idle":"2024-05-29T16:22:48.147936Z","shell.execute_reply.started":"2024-05-29T16:22:48.130179Z","shell.execute_reply":"2024-05-29T16:22:48.146889Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5646 entries, 0 to 5645\nColumns: 151 entries, patient_id to Average of Dec-18\ndtypes: float64(137), int64(3), object(11)\nmemory usage: 6.5+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"num_rows_shape = testdf.shape[0]\nprint(\"Number of rows in DataFrame (using shape):\", num_rows_shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:22:55.408234Z","iopub.execute_input":"2024-05-29T16:22:55.408644Z","iopub.status.idle":"2024-05-29T16:22:55.415194Z","shell.execute_reply.started":"2024-05-29T16:22:55.408591Z","shell.execute_reply":"2024-05-29T16:22:55.413947Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Number of rows in DataFrame (using shape): 5646\n","output_type":"stream"}]},{"cell_type":"code","source":"#Check out the DF\ntraindf.describe()\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nvariable_types = traindf.dtypes\nprint(variable_types)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T15:28:27.147877Z","iopub.execute_input":"2024-05-29T15:28:27.148248Z","iopub.status.idle":"2024-05-29T15:28:27.493154Z","shell.execute_reply.started":"2024-05-29T15:28:27.148220Z","shell.execute_reply":"2024-05-29T15:28:27.491933Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"patient_id                                 int64\npatient_race                              object\npayer_type                                object\npatient_state                             object\npatient_zip3                               int64\nRegion                                    object\nDivision                                  object\npatient_age                                int64\npatient_gender                            object\nbmi                                      float64\nbreast_cancer_diagnosis_code              object\nbreast_cancer_diagnosis_desc              object\nmetastatic_cancer_diagnosis_code          object\nmetastatic_first_novel_treatment          object\nmetastatic_first_novel_treatment_type     object\npopulation                               float64\ndensity                                  float64\nage_median                               float64\nage_under_10                             float64\nage_10_to_19                             float64\nage_20s                                  float64\nage_30s                                  float64\nage_40s                                  float64\nage_50s                                  float64\nage_60s                                  float64\nage_70s                                  float64\nage_over_80                              float64\nmale                                     float64\nfemale                                   float64\nmarried                                  float64\ndivorced                                 float64\nnever_married                            float64\nwidowed                                  float64\nfamily_size                              float64\nfamily_dual_income                       float64\nincome_household_median                  float64\nincome_household_under_5                 float64\nincome_household_5_to_10                 float64\nincome_household_10_to_15                float64\nincome_household_15_to_20                float64\nincome_household_20_to_25                float64\nincome_household_25_to_35                float64\nincome_household_35_to_50                float64\nincome_household_50_to_75                float64\nincome_household_75_to_100               float64\nincome_household_100_to_150              float64\nincome_household_150_over                float64\nincome_household_six_figure              float64\nincome_individual_median                 float64\nhome_ownership                           float64\nhousing_units                            float64\nhome_value                               float64\nrent_median                              float64\nrent_burden                              float64\neducation_less_highschool                float64\neducation_highschool                     float64\neducation_some_college                   float64\neducation_bachelors                      float64\neducation_graduate                       float64\neducation_college_or_above               float64\neducation_stem_degree                    float64\nlabor_force_participation                float64\nunemployment_rate                        float64\nself_employed                            float64\nfarmer                                   float64\nrace_white                               float64\nrace_black                               float64\nrace_asian                               float64\nrace_native                              float64\nrace_pacific                             float64\nrace_other                               float64\nrace_multiple                            float64\nhispanic                                 float64\ndisabled                                 float64\npoverty                                  float64\nlimited_english                          float64\ncommute_time                             float64\nhealth_uninsured                         float64\nveteran                                  float64\nAverage of Jan-13                        float64\nAverage of Feb-13                        float64\nAverage of Mar-13                        float64\nAverage of Apr-13                        float64\nAverage of May-13                        float64\nAverage of Jun-13                        float64\nAverage of Jul-13                        float64\nAverage of Aug-13                        float64\nAverage of Sep-13                        float64\nAverage of Oct-13                        float64\nAverage of Nov-13                        float64\nAverage of Dec-13                        float64\nAverage of Jan-14                        float64\nAverage of Feb-14                        float64\nAverage of Mar-14                        float64\nAverage of Apr-14                        float64\nAverage of May-14                        float64\nAverage of Jun-14                        float64\nAverage of Jul-14                        float64\nAverage of Aug-14                        float64\nAverage of Sep-14                        float64\nAverage of Oct-14                        float64\nAverage of Nov-14                        float64\nAverage of Dec-14                        float64\nAverage of Jan-15                        float64\nAverage of Feb-15                        float64\nAverage of Mar-15                        float64\nAverage of Apr-15                        float64\nAverage of May-15                        float64\nAverage of Jun-15                        float64\nAverage of Jul-15                        float64\nAverage of Aug-15                        float64\nAverage of Sep-15                        float64\nAverage of Oct-15                        float64\nAverage of Nov-15                        float64\nAverage of Dec-15                        float64\nAverage of Jan-16                        float64\nAverage of Feb-16                        float64\nAverage of Mar-16                        float64\nAverage of Apr-16                        float64\nAverage of May-16                        float64\nAverage of Jun-16                        float64\nAverage of Jul-16                        float64\nAverage of Aug-16                        float64\nAverage of Sep-16                        float64\nAverage of Oct-16                        float64\nAverage of Nov-16                        float64\nAverage of Dec-16                        float64\nAverage of Jan-17                        float64\nAverage of Feb-17                        float64\nAverage of Mar-17                        float64\nAverage of Apr-17                        float64\nAverage of May-17                        float64\nAverage of Jun-17                        float64\nAverage of Jul-17                        float64\nAverage of Aug-17                        float64\nAverage of Sep-17                        float64\nAverage of Oct-17                        float64\nAverage of Nov-17                        float64\nAverage of Dec-17                        float64\nAverage of Jan-18                        float64\nAverage of Feb-18                        float64\nAverage of Mar-18                        float64\nAverage of Apr-18                        float64\nAverage of May-18                        float64\nAverage of Jun-18                        float64\nAverage of Jul-18                        float64\nAverage of Aug-18                        float64\nAverage of Sep-18                        float64\nAverage of Oct-18                        float64\nAverage of Nov-18                        float64\nAverage of Dec-18                        float64\nmetastatic_diagnosis_period                int64\ndtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"#Check out the DF\ntestdf.describe()\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nvariable_types = traindf.dtypes\nprint(variable_types)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T15:30:30.884712Z","iopub.execute_input":"2024-05-29T15:30:30.885126Z","iopub.status.idle":"2024-05-29T15:30:31.177160Z","shell.execute_reply.started":"2024-05-29T15:30:30.885096Z","shell.execute_reply":"2024-05-29T15:30:31.175860Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"patient_id                                 int64\npatient_race                              object\npayer_type                                object\npatient_state                             object\npatient_zip3                               int64\nRegion                                    object\nDivision                                  object\npatient_age                                int64\npatient_gender                            object\nbmi                                      float64\nbreast_cancer_diagnosis_code              object\nbreast_cancer_diagnosis_desc              object\nmetastatic_cancer_diagnosis_code          object\nmetastatic_first_novel_treatment          object\nmetastatic_first_novel_treatment_type     object\npopulation                               float64\ndensity                                  float64\nage_median                               float64\nage_under_10                             float64\nage_10_to_19                             float64\nage_20s                                  float64\nage_30s                                  float64\nage_40s                                  float64\nage_50s                                  float64\nage_60s                                  float64\nage_70s                                  float64\nage_over_80                              float64\nmale                                     float64\nfemale                                   float64\nmarried                                  float64\ndivorced                                 float64\nnever_married                            float64\nwidowed                                  float64\nfamily_size                              float64\nfamily_dual_income                       float64\nincome_household_median                  float64\nincome_household_under_5                 float64\nincome_household_5_to_10                 float64\nincome_household_10_to_15                float64\nincome_household_15_to_20                float64\nincome_household_20_to_25                float64\nincome_household_25_to_35                float64\nincome_household_35_to_50                float64\nincome_household_50_to_75                float64\nincome_household_75_to_100               float64\nincome_household_100_to_150              float64\nincome_household_150_over                float64\nincome_household_six_figure              float64\nincome_individual_median                 float64\nhome_ownership                           float64\nhousing_units                            float64\nhome_value                               float64\nrent_median                              float64\nrent_burden                              float64\neducation_less_highschool                float64\neducation_highschool                     float64\neducation_some_college                   float64\neducation_bachelors                      float64\neducation_graduate                       float64\neducation_college_or_above               float64\neducation_stem_degree                    float64\nlabor_force_participation                float64\nunemployment_rate                        float64\nself_employed                            float64\nfarmer                                   float64\nrace_white                               float64\nrace_black                               float64\nrace_asian                               float64\nrace_native                              float64\nrace_pacific                             float64\nrace_other                               float64\nrace_multiple                            float64\nhispanic                                 float64\ndisabled                                 float64\npoverty                                  float64\nlimited_english                          float64\ncommute_time                             float64\nhealth_uninsured                         float64\nveteran                                  float64\nAverage of Jan-13                        float64\nAverage of Feb-13                        float64\nAverage of Mar-13                        float64\nAverage of Apr-13                        float64\nAverage of May-13                        float64\nAverage of Jun-13                        float64\nAverage of Jul-13                        float64\nAverage of Aug-13                        float64\nAverage of Sep-13                        float64\nAverage of Oct-13                        float64\nAverage of Nov-13                        float64\nAverage of Dec-13                        float64\nAverage of Jan-14                        float64\nAverage of Feb-14                        float64\nAverage of Mar-14                        float64\nAverage of Apr-14                        float64\nAverage of May-14                        float64\nAverage of Jun-14                        float64\nAverage of Jul-14                        float64\nAverage of Aug-14                        float64\nAverage of Sep-14                        float64\nAverage of Oct-14                        float64\nAverage of Nov-14                        float64\nAverage of Dec-14                        float64\nAverage of Jan-15                        float64\nAverage of Feb-15                        float64\nAverage of Mar-15                        float64\nAverage of Apr-15                        float64\nAverage of May-15                        float64\nAverage of Jun-15                        float64\nAverage of Jul-15                        float64\nAverage of Aug-15                        float64\nAverage of Sep-15                        float64\nAverage of Oct-15                        float64\nAverage of Nov-15                        float64\nAverage of Dec-15                        float64\nAverage of Jan-16                        float64\nAverage of Feb-16                        float64\nAverage of Mar-16                        float64\nAverage of Apr-16                        float64\nAverage of May-16                        float64\nAverage of Jun-16                        float64\nAverage of Jul-16                        float64\nAverage of Aug-16                        float64\nAverage of Sep-16                        float64\nAverage of Oct-16                        float64\nAverage of Nov-16                        float64\nAverage of Dec-16                        float64\nAverage of Jan-17                        float64\nAverage of Feb-17                        float64\nAverage of Mar-17                        float64\nAverage of Apr-17                        float64\nAverage of May-17                        float64\nAverage of Jun-17                        float64\nAverage of Jul-17                        float64\nAverage of Aug-17                        float64\nAverage of Sep-17                        float64\nAverage of Oct-17                        float64\nAverage of Nov-17                        float64\nAverage of Dec-17                        float64\nAverage of Jan-18                        float64\nAverage of Feb-18                        float64\nAverage of Mar-18                        float64\nAverage of Apr-18                        float64\nAverage of May-18                        float64\nAverage of Jun-18                        float64\nAverage of Jul-18                        float64\nAverage of Aug-18                        float64\nAverage of Sep-18                        float64\nAverage of Oct-18                        float64\nAverage of Nov-18                        float64\nAverage of Dec-18                        float64\nmetastatic_diagnosis_period                int64\ndtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"#Delete excessive weather notes\ndeletevar13 = ['Average of Mar-13', 'Average of Apr-13', 'Average of May-13', 'Average of Jun-13', 'Average of Aug-13', 'Average of Sep-13', 'Average of Oct-13', 'Average of Nov-13', 'Average of Dec-13']\ntraindf =traindf.drop(deletevar13, axis=1)\ndeletevar14 = ['Average of Feb-14', 'Average of Mar-14', 'Average of Apr-14', 'Average of May-14', 'Average of Jun-14', 'Average of Aug-14', 'Average of Sep-14', 'Average of Oct-14', 'Average of Nov-14', 'Average of Dec-14']\ntraindf =traindf.drop(deletevar14, axis=1)\ndeletevar15 = ['Average of Feb-15', 'Average of Mar-15', 'Average of Apr-15', 'Average of May-15', 'Average of Jun-15', 'Average of Aug-15', 'Average of Sep-15', 'Average of Oct-15', 'Average of Nov-15', 'Average of Dec-15']\ntraindf =traindf.drop(deletevar15, axis=1)\ndeletevar16 = ['Average of Jan-16','Average of Feb-16','Average of Feb-16', 'Average of Mar-16', 'Average of Apr-16', 'Average of May-16', 'Average of Jul-16', 'Average of Aug-16', 'Average of Sep-16', 'Average of Oct-16', 'Average of Nov-16', 'Average of Dec-16']\ntraindf =traindf.drop(deletevar16, axis=1)\ndeletevar17 = ['Average of Feb-17', 'Average of Mar-17', 'Average of Apr-17', 'Average of May-17', 'Average of Jun-17', 'Average of Aug-17', 'Average of Sep-17', 'Average of Oct-17', 'Average of Nov-17', 'Average of Dec-17']\ntraindf =traindf.drop(deletevar17, axis=1)\ndeletevar18 = ['Average of Feb-18', 'Average of Mar-18', 'Average of Apr-18', 'Average of May-18', 'Average of Jun-18', 'Average of Aug-18', 'Average of Sep-18', 'Average of Oct-18', 'Average of Nov-18', 'Average of Dec-18']\ntraindf =traindf.drop(deletevar18, axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:35:14.422109Z","iopub.execute_input":"2024-05-29T16:35:14.422521Z","iopub.status.idle":"2024-05-29T16:35:14.463225Z","shell.execute_reply.started":"2024-05-29T16:35:14.422488Z","shell.execute_reply":"2024-05-29T16:35:14.461880Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"# Check out missing values\nmissing_values = traindf.isnull().sum()\nmissing_values_df = missing_values.reset_index()\nmissing_values_df.columns = ['Column', 'Missing Values']\npd.set_option('display.max_columns', None)\nfor column, count in missing_values.items():\n    print(f\"Column {column} has {count} missing values\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T15:30:49.548287Z","iopub.execute_input":"2024-05-29T15:30:49.548684Z","iopub.status.idle":"2024-05-29T15:30:49.568116Z","shell.execute_reply.started":"2024-05-29T15:30:49.548653Z","shell.execute_reply":"2024-05-29T15:30:49.567048Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Column patient_id has 0 missing values\nColumn patient_race has 6657 missing values\nColumn payer_type has 1765 missing values\nColumn patient_state has 0 missing values\nColumn patient_zip3 has 0 missing values\nColumn Region has 0 missing values\nColumn Division has 0 missing values\nColumn patient_age has 0 missing values\nColumn patient_gender has 0 missing values\nColumn bmi has 9071 missing values\nColumn breast_cancer_diagnosis_code has 0 missing values\nColumn breast_cancer_diagnosis_desc has 0 missing values\nColumn metastatic_cancer_diagnosis_code has 0 missing values\nColumn metastatic_first_novel_treatment has 13162 missing values\nColumn metastatic_first_novel_treatment_type has 13162 missing values\nColumn population has 0 missing values\nColumn density has 0 missing values\nColumn age_median has 0 missing values\nColumn age_under_10 has 0 missing values\nColumn age_10_to_19 has 0 missing values\nColumn age_20s has 0 missing values\nColumn age_30s has 0 missing values\nColumn age_40s has 0 missing values\nColumn age_50s has 0 missing values\nColumn age_60s has 0 missing values\nColumn age_70s has 0 missing values\nColumn age_over_80 has 0 missing values\nColumn male has 0 missing values\nColumn female has 0 missing values\nColumn married has 0 missing values\nColumn divorced has 0 missing values\nColumn never_married has 0 missing values\nColumn widowed has 0 missing values\nColumn family_size has 5 missing values\nColumn family_dual_income has 5 missing values\nColumn income_household_median has 5 missing values\nColumn income_household_under_5 has 5 missing values\nColumn income_household_5_to_10 has 5 missing values\nColumn income_household_10_to_15 has 5 missing values\nColumn income_household_15_to_20 has 5 missing values\nColumn income_household_20_to_25 has 5 missing values\nColumn income_household_25_to_35 has 5 missing values\nColumn income_household_35_to_50 has 5 missing values\nColumn income_household_50_to_75 has 5 missing values\nColumn income_household_75_to_100 has 5 missing values\nColumn income_household_100_to_150 has 5 missing values\nColumn income_household_150_over has 5 missing values\nColumn income_household_six_figure has 5 missing values\nColumn income_individual_median has 0 missing values\nColumn home_ownership has 5 missing values\nColumn housing_units has 0 missing values\nColumn home_value has 5 missing values\nColumn rent_median has 5 missing values\nColumn rent_burden has 5 missing values\nColumn education_less_highschool has 0 missing values\nColumn education_highschool has 0 missing values\nColumn education_some_college has 0 missing values\nColumn education_bachelors has 0 missing values\nColumn education_graduate has 0 missing values\nColumn education_college_or_above has 0 missing values\nColumn education_stem_degree has 0 missing values\nColumn labor_force_participation has 0 missing values\nColumn unemployment_rate has 0 missing values\nColumn self_employed has 5 missing values\nColumn farmer has 5 missing values\nColumn race_white has 0 missing values\nColumn race_black has 0 missing values\nColumn race_asian has 0 missing values\nColumn race_native has 0 missing values\nColumn race_pacific has 0 missing values\nColumn race_other has 0 missing values\nColumn race_multiple has 0 missing values\nColumn hispanic has 0 missing values\nColumn disabled has 0 missing values\nColumn poverty has 5 missing values\nColumn limited_english has 5 missing values\nColumn commute_time has 0 missing values\nColumn health_uninsured has 0 missing values\nColumn veteran has 0 missing values\nColumn Average of Jan-13 has 33 missing values\nColumn Average of Feb-13 has 3 missing values\nColumn Average of Mar-13 has 0 missing values\nColumn Average of Apr-13 has 0 missing values\nColumn Average of May-13 has 3 missing values\nColumn Average of Jun-13 has 20 missing values\nColumn Average of Jul-13 has 0 missing values\nColumn Average of Aug-13 has 17 missing values\nColumn Average of Sep-13 has 27 missing values\nColumn Average of Oct-13 has 59 missing values\nColumn Average of Nov-13 has 3 missing values\nColumn Average of Dec-13 has 3 missing values\nColumn Average of Jan-14 has 4 missing values\nColumn Average of Feb-14 has 9 missing values\nColumn Average of Mar-14 has 29 missing values\nColumn Average of Apr-14 has 180 missing values\nColumn Average of May-14 has 0 missing values\nColumn Average of Jun-14 has 152 missing values\nColumn Average of Jul-14 has 0 missing values\nColumn Average of Aug-14 has 0 missing values\nColumn Average of Sep-14 has 0 missing values\nColumn Average of Oct-14 has 0 missing values\nColumn Average of Nov-14 has 24 missing values\nColumn Average of Dec-14 has 0 missing values\nColumn Average of Jan-15 has 6 missing values\nColumn Average of Feb-15 has 12 missing values\nColumn Average of Mar-15 has 12 missing values\nColumn Average of Apr-15 has 28 missing values\nColumn Average of May-15 has 0 missing values\nColumn Average of Jun-15 has 0 missing values\nColumn Average of Jul-15 has 0 missing values\nColumn Average of Aug-15 has 22 missing values\nColumn Average of Sep-15 has 0 missing values\nColumn Average of Oct-15 has 16 missing values\nColumn Average of Nov-15 has 16 missing values\nColumn Average of Dec-15 has 18 missing values\nColumn Average of Jan-16 has 16 missing values\nColumn Average of Feb-16 has 16 missing values\nColumn Average of Mar-16 has 0 missing values\nColumn Average of Apr-16 has 0 missing values\nColumn Average of May-16 has 19 missing values\nColumn Average of Jun-16 has 0 missing values\nColumn Average of Jul-16 has 16 missing values\nColumn Average of Aug-16 has 0 missing values\nColumn Average of Sep-16 has 0 missing values\nColumn Average of Oct-16 has 0 missing values\nColumn Average of Nov-16 has 3 missing values\nColumn Average of Dec-16 has 13 missing values\nColumn Average of Jan-17 has 9 missing values\nColumn Average of Feb-17 has 0 missing values\nColumn Average of Mar-17 has 0 missing values\nColumn Average of Apr-17 has 0 missing values\nColumn Average of May-17 has 0 missing values\nColumn Average of Jun-17 has 1 missing values\nColumn Average of Jul-17 has 31 missing values\nColumn Average of Aug-17 has 0 missing values\nColumn Average of Sep-17 has 10 missing values\nColumn Average of Oct-17 has 21 missing values\nColumn Average of Nov-17 has 5 missing values\nColumn Average of Dec-17 has 0 missing values\nColumn Average of Jan-18 has 0 missing values\nColumn Average of Feb-18 has 5 missing values\nColumn Average of Mar-18 has 6 missing values\nColumn Average of Apr-18 has 0 missing values\nColumn Average of May-18 has 0 missing values\nColumn Average of Jun-18 has 9 missing values\nColumn Average of Jul-18 has 46 missing values\nColumn Average of Aug-18 has 16 missing values\nColumn Average of Sep-18 has 7 missing values\nColumn Average of Oct-18 has 7 missing values\nColumn Average of Nov-18 has 12 missing values\nColumn Average of Dec-18 has 33 missing values\nColumn metastatic_diagnosis_period has 0 missing values\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check out missing values\nmissing_values = testdf.isnull().sum()\nmissing_values_df = missing_values.reset_index()\nmissing_values_df.columns = ['Column', 'Missing Values']\npd.set_option('display.max_columns', None)\nfor column, count in missing_values.items():\n    print(f\"Column {column} has {count} missing values\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T15:31:10.126440Z","iopub.execute_input":"2024-05-29T15:31:10.126838Z","iopub.status.idle":"2024-05-29T15:31:10.141395Z","shell.execute_reply.started":"2024-05-29T15:31:10.126806Z","shell.execute_reply":"2024-05-29T15:31:10.139813Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Column patient_id has 0 missing values\nColumn patient_race has 2785 missing values\nColumn payer_type has 785 missing values\nColumn patient_state has 0 missing values\nColumn patient_zip3 has 0 missing values\nColumn Region has 0 missing values\nColumn Division has 0 missing values\nColumn patient_age has 0 missing values\nColumn patient_gender has 0 missing values\nColumn bmi has 3941 missing values\nColumn breast_cancer_diagnosis_code has 0 missing values\nColumn breast_cancer_diagnosis_desc has 0 missing values\nColumn metastatic_cancer_diagnosis_code has 0 missing values\nColumn metastatic_first_novel_treatment has 5639 missing values\nColumn metastatic_first_novel_treatment_type has 5639 missing values\nColumn population has 0 missing values\nColumn density has 0 missing values\nColumn age_median has 0 missing values\nColumn age_under_10 has 0 missing values\nColumn age_10_to_19 has 0 missing values\nColumn age_20s has 0 missing values\nColumn age_30s has 0 missing values\nColumn age_40s has 0 missing values\nColumn age_50s has 0 missing values\nColumn age_60s has 0 missing values\nColumn age_70s has 0 missing values\nColumn age_over_80 has 0 missing values\nColumn male has 0 missing values\nColumn female has 0 missing values\nColumn married has 0 missing values\nColumn divorced has 0 missing values\nColumn never_married has 0 missing values\nColumn widowed has 0 missing values\nColumn family_size has 0 missing values\nColumn family_dual_income has 0 missing values\nColumn income_household_median has 0 missing values\nColumn income_household_under_5 has 0 missing values\nColumn income_household_5_to_10 has 0 missing values\nColumn income_household_10_to_15 has 0 missing values\nColumn income_household_15_to_20 has 0 missing values\nColumn income_household_20_to_25 has 0 missing values\nColumn income_household_25_to_35 has 0 missing values\nColumn income_household_35_to_50 has 0 missing values\nColumn income_household_50_to_75 has 0 missing values\nColumn income_household_75_to_100 has 0 missing values\nColumn income_household_100_to_150 has 0 missing values\nColumn income_household_150_over has 0 missing values\nColumn income_household_six_figure has 0 missing values\nColumn income_individual_median has 0 missing values\nColumn home_ownership has 0 missing values\nColumn housing_units has 0 missing values\nColumn home_value has 0 missing values\nColumn rent_median has 0 missing values\nColumn rent_burden has 0 missing values\nColumn education_less_highschool has 0 missing values\nColumn education_highschool has 0 missing values\nColumn education_some_college has 0 missing values\nColumn education_bachelors has 0 missing values\nColumn education_graduate has 0 missing values\nColumn education_college_or_above has 0 missing values\nColumn education_stem_degree has 0 missing values\nColumn labor_force_participation has 0 missing values\nColumn unemployment_rate has 0 missing values\nColumn self_employed has 0 missing values\nColumn farmer has 0 missing values\nColumn race_white has 0 missing values\nColumn race_black has 0 missing values\nColumn race_asian has 0 missing values\nColumn race_native has 0 missing values\nColumn race_pacific has 0 missing values\nColumn race_other has 0 missing values\nColumn race_multiple has 0 missing values\nColumn hispanic has 0 missing values\nColumn disabled has 0 missing values\nColumn poverty has 0 missing values\nColumn limited_english has 0 missing values\nColumn commute_time has 0 missing values\nColumn health_uninsured has 0 missing values\nColumn veteran has 0 missing values\nColumn Average of Jan-13 has 9 missing values\nColumn Average of Feb-13 has 0 missing values\nColumn Average of Mar-13 has 0 missing values\nColumn Average of Apr-13 has 0 missing values\nColumn Average of May-13 has 0 missing values\nColumn Average of Jun-13 has 7 missing values\nColumn Average of Jul-13 has 0 missing values\nColumn Average of Aug-13 has 5 missing values\nColumn Average of Sep-13 has 14 missing values\nColumn Average of Oct-13 has 27 missing values\nColumn Average of Nov-13 has 0 missing values\nColumn Average of Dec-13 has 0 missing values\nColumn Average of Jan-14 has 3 missing values\nColumn Average of Feb-14 has 6 missing values\nColumn Average of Mar-14 has 17 missing values\nColumn Average of Apr-14 has 95 missing values\nColumn Average of May-14 has 0 missing values\nColumn Average of Jun-14 has 79 missing values\nColumn Average of Jul-14 has 0 missing values\nColumn Average of Aug-14 has 0 missing values\nColumn Average of Sep-14 has 0 missing values\nColumn Average of Oct-14 has 0 missing values\nColumn Average of Nov-14 has 9 missing values\nColumn Average of Dec-14 has 0 missing values\nColumn Average of Jan-15 has 3 missing values\nColumn Average of Feb-15 has 6 missing values\nColumn Average of Mar-15 has 6 missing values\nColumn Average of Apr-15 has 10 missing values\nColumn Average of May-15 has 0 missing values\nColumn Average of Jun-15 has 0 missing values\nColumn Average of Jul-15 has 0 missing values\nColumn Average of Aug-15 has 15 missing values\nColumn Average of Sep-15 has 0 missing values\nColumn Average of Oct-15 has 4 missing values\nColumn Average of Nov-15 has 4 missing values\nColumn Average of Dec-15 has 5 missing values\nColumn Average of Jan-16 has 4 missing values\nColumn Average of Feb-16 has 4 missing values\nColumn Average of Mar-16 has 0 missing values\nColumn Average of Apr-16 has 0 missing values\nColumn Average of May-16 has 4 missing values\nColumn Average of Jun-16 has 0 missing values\nColumn Average of Jul-16 has 4 missing values\nColumn Average of Aug-16 has 0 missing values\nColumn Average of Sep-16 has 0 missing values\nColumn Average of Oct-16 has 0 missing values\nColumn Average of Nov-16 has 0 missing values\nColumn Average of Dec-16 has 2 missing values\nColumn Average of Jan-17 has 3 missing values\nColumn Average of Feb-17 has 0 missing values\nColumn Average of Mar-17 has 0 missing values\nColumn Average of Apr-17 has 0 missing values\nColumn Average of May-17 has 0 missing values\nColumn Average of Jun-17 has 1 missing values\nColumn Average of Jul-17 has 13 missing values\nColumn Average of Aug-17 has 0 missing values\nColumn Average of Sep-17 has 2 missing values\nColumn Average of Oct-17 has 15 missing values\nColumn Average of Nov-17 has 6 missing values\nColumn Average of Dec-17 has 0 missing values\nColumn Average of Jan-18 has 0 missing values\nColumn Average of Feb-18 has 6 missing values\nColumn Average of Mar-18 has 7 missing values\nColumn Average of Apr-18 has 0 missing values\nColumn Average of May-18 has 0 missing values\nColumn Average of Jun-18 has 5 missing values\nColumn Average of Jul-18 has 16 missing values\nColumn Average of Aug-18 has 7 missing values\nColumn Average of Sep-18 has 2 missing values\nColumn Average of Oct-18 has 2 missing values\nColumn Average of Nov-18 has 8 missing values\nColumn Average of Dec-18 has 23 missing values\n","output_type":"stream"}]},{"cell_type":"code","source":"#Delete excessive weather notes\ndeletevar13 = ['Average of Mar-13', 'Average of Apr-13', 'Average of May-13', 'Average of Jun-13', 'Average of Aug-13', 'Average of Sep-13', 'Average of Oct-13', 'Average of Nov-13', 'Average of Dec-13']\ntestdf =testdf.drop(deletevar13, axis=1)\ndeletevar14 = ['Average of Feb-14', 'Average of Mar-14', 'Average of Apr-14', 'Average of May-14', 'Average of Jun-14', 'Average of Aug-14', 'Average of Sep-14', 'Average of Oct-14', 'Average of Nov-14', 'Average of Dec-14']\ntestdf =testdf.drop(deletevar14, axis=1)\ndeletevar15 = ['Average of Feb-15', 'Average of Mar-15', 'Average of Apr-15', 'Average of May-15', 'Average of Jun-15', 'Average of Aug-15', 'Average of Sep-15', 'Average of Oct-15', 'Average of Nov-15', 'Average of Dec-15']\ntestdf =testdf.drop(deletevar15, axis=1)\ndeletevar16 = ['Average of Jan-16','Average of Feb-16','Average of Feb-16', 'Average of Mar-16', 'Average of Apr-16', 'Average of May-16', 'Average of Jun-16', 'Average of Aug-16', 'Average of Sep-16', 'Average of Oct-16', 'Average of Nov-16', 'Average of Dec-16']\ntestdf =testdf.drop(deletevar16, axis=1)\ndeletevar17 = ['Average of Feb-17', 'Average of Mar-17', 'Average of Apr-17', 'Average of Jun-17', 'Average of Jul-17','Average of Aug-17', 'Average of Sep-17', 'Average of Oct-17', 'Average of Nov-17', 'Average of Dec-17']\ntestdf =testdf.drop(deletevar17, axis=1)\ndeletevar18 = ['Average of Feb-18', 'Average of Mar-18', 'Average of Apr-18', 'Average of Jun-18', 'Average of Aug-18', 'Average of Sep-18', 'Average of Oct-18', 'Average of Nov-18', 'Average of Dec-18']\ntestdf =testdf.drop(deletevar18, axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:35:25.706501Z","iopub.execute_input":"2024-05-29T16:35:25.706960Z","iopub.status.idle":"2024-05-29T16:35:25.732167Z","shell.execute_reply.started":"2024-05-29T16:35:25.706927Z","shell.execute_reply":"2024-05-29T16:35:25.731022Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"num_rows_shape = testdf.shape[0]\nprint(\"Number of rows in DataFrame (using shape):\", num_rows_shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:35:30.704549Z","iopub.execute_input":"2024-05-29T16:35:30.704967Z","iopub.status.idle":"2024-05-29T16:35:30.711662Z","shell.execute_reply.started":"2024-05-29T16:35:30.704934Z","shell.execute_reply":"2024-05-29T16:35:30.709788Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"Number of rows in DataFrame (using shape): 5646\n","output_type":"stream"}]},{"cell_type":"code","source":"#Delete variables I don’t need\ntraindf.drop(columns= ['patient_state', 'Region', 'Division', 'patient_gender'], inplace=True)\ntraindf.drop(columns= ['bmi', 'breast_cancer_diagnosis_desc', 'breast_cancer_diagnosis_code', 'breast_cancer_diagnosis_code', 'metastatic_cancer_diagnosis_code', 'metastatic_first_novel_treatment', 'metastatic_first_novel_treatment_type'], inplace=True)\n\ntraindf.drop (columns=['family_size', 'family_dual_income', 'income_household_median', 'income_household_under_5', 'income_household_5_to_10','income_household_10_to_15', 'income_household_15_to_20', 'income_household_20_to_25','income_household_25_to_35', 'income_household_35_to_50', 'income_household_50_to_75', 'income_household_75_to_100', 'income_household_100_to_150', 'income_household_150_over', 'income_household_six_figure'], inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:00:05.069663Z","iopub.execute_input":"2024-05-31T23:00:05.070050Z","iopub.status.idle":"2024-05-31T23:00:05.118954Z","shell.execute_reply.started":"2024-05-31T23:00:05.070015Z","shell.execute_reply":"2024-05-31T23:00:05.117780Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"testdf.drop(columns= ['Average of Jul-16', 'Average of May-17', 'Average of May-18'], inplace=True)\ntraindf.drop(columns= ['Average of Jun-16', 'Average of Jul-17'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:35:51.865856Z","iopub.execute_input":"2024-05-29T16:35:51.866285Z","iopub.status.idle":"2024-05-29T16:35:51.878121Z","shell.execute_reply.started":"2024-05-29T16:35:51.866251Z","shell.execute_reply":"2024-05-29T16:35:51.876993Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"num_rows_shape = testdf.shape[0]\nprint(\"Number of rows in DataFrame (using shape):\", num_rows_shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:35:56.025717Z","iopub.execute_input":"2024-05-29T16:35:56.026387Z","iopub.status.idle":"2024-05-29T16:35:56.031664Z","shell.execute_reply.started":"2024-05-29T16:35:56.026353Z","shell.execute_reply":"2024-05-29T16:35:56.030533Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"Number of rows in DataFrame (using shape): 5646\n","output_type":"stream"}]},{"cell_type":"code","source":"#Delete variables I don’t need\ntestdf.drop(columns= ['patient_state', 'Region', 'Division', 'patient_gender'], inplace=True)\ntestdf.drop(columns= ['bmi', 'breast_cancer_diagnosis_desc', 'breast_cancer_diagnosis_code', 'breast_cancer_diagnosis_code', 'metastatic_cancer_diagnosis_code', 'metastatic_first_novel_treatment', 'metastatic_first_novel_treatment_type'], inplace=True)\n\ntestdf.drop (columns=['family_size', 'family_dual_income', 'income_household_median', 'income_household_under_5', 'income_household_5_to_10','income_household_10_to_15', 'income_household_15_to_20', 'income_household_20_to_25','income_household_25_to_35', 'income_household_35_to_50', 'income_household_50_to_75', 'income_household_75_to_100', 'income_household_100_to_150', 'income_household_150_over', 'income_household_six_figure'], inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:00:14.289103Z","iopub.execute_input":"2024-05-31T23:00:14.289489Z","iopub.status.idle":"2024-05-31T23:00:14.308249Z","shell.execute_reply.started":"2024-05-31T23:00:14.289451Z","shell.execute_reply":"2024-05-31T23:00:14.307007Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"num_rows_shape = testdf.shape[0]\nprint(\"Number of rows in DataFrame (using shape):\", num_rows_shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:36:04.245781Z","iopub.execute_input":"2024-05-29T16:36:04.246173Z","iopub.status.idle":"2024-05-29T16:36:04.252014Z","shell.execute_reply.started":"2024-05-29T16:36:04.246140Z","shell.execute_reply":"2024-05-29T16:36:04.250842Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"Number of rows in DataFrame (using shape): 5646\n","output_type":"stream"}]},{"cell_type":"code","source":"#Encode missing values\ntraindf['patient_race'].fillna('Unknown', inplace=True)\ntraindf['payer_type'].fillna('Unknown', inplace=True)\ntestdf['patient_race'].fillna('Unknown', inplace=True)\ntestdf['payer_type'].fillna('Unknown', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:00:22.994766Z","iopub.execute_input":"2024-05-31T23:00:22.995197Z","iopub.status.idle":"2024-05-31T23:00:23.008871Z","shell.execute_reply.started":"2024-05-31T23:00:22.995159Z","shell.execute_reply":"2024-05-31T23:00:23.007450Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2488444906.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  traindf['patient_race'].fillna('Unknown', inplace=True)\n/tmp/ipykernel_34/2488444906.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  traindf['payer_type'].fillna('Unknown', inplace=True)\n/tmp/ipykernel_34/2488444906.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  testdf['patient_race'].fillna('Unknown', inplace=True)\n/tmp/ipykernel_34/2488444906.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  testdf['payer_type'].fillna('Unknown', inplace=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"num_rows_shape = testdf.shape[0]\nprint(\"Number of rows in DataFrame (using shape):\", num_rows_shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:36:18.112847Z","iopub.execute_input":"2024-05-29T16:36:18.113235Z","iopub.status.idle":"2024-05-29T16:36:18.119934Z","shell.execute_reply.started":"2024-05-29T16:36:18.113207Z","shell.execute_reply":"2024-05-29T16:36:18.118515Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"Number of rows in DataFrame (using shape): 5646\n","output_type":"stream"}]},{"cell_type":"code","source":"#Check out missing again\nmissing_values = traindf.isnull().sum()\nmissing_values_df = missing_values.reset_index()\nmissing_values_df.columns = ['Column', 'Missing Values']\npd.set_option('display.max_columns', None)\nfor column, count in missing_values.items():\n    print(f\"Column {column} has {count} missing values\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:00:54.343665Z","iopub.execute_input":"2024-05-31T23:00:54.344903Z","iopub.status.idle":"2024-05-31T23:00:54.361458Z","shell.execute_reply.started":"2024-05-31T23:00:54.344860Z","shell.execute_reply":"2024-05-31T23:00:54.359854Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Column patient_id has 0 missing values\nColumn patient_race has 0 missing values\nColumn payer_type has 0 missing values\nColumn patient_zip3 has 0 missing values\nColumn patient_age has 0 missing values\nColumn population has 0 missing values\nColumn density has 0 missing values\nColumn age_median has 0 missing values\nColumn age_under_10 has 0 missing values\nColumn age_10_to_19 has 0 missing values\nColumn age_20s has 0 missing values\nColumn age_30s has 0 missing values\nColumn age_40s has 0 missing values\nColumn age_50s has 0 missing values\nColumn age_60s has 0 missing values\nColumn age_70s has 0 missing values\nColumn age_over_80 has 0 missing values\nColumn male has 0 missing values\nColumn female has 0 missing values\nColumn married has 0 missing values\nColumn divorced has 0 missing values\nColumn never_married has 0 missing values\nColumn widowed has 0 missing values\nColumn income_individual_median has 0 missing values\nColumn home_ownership has 5 missing values\nColumn housing_units has 0 missing values\nColumn home_value has 5 missing values\nColumn rent_median has 5 missing values\nColumn rent_burden has 5 missing values\nColumn education_less_highschool has 0 missing values\nColumn education_highschool has 0 missing values\nColumn education_some_college has 0 missing values\nColumn education_bachelors has 0 missing values\nColumn education_graduate has 0 missing values\nColumn education_college_or_above has 0 missing values\nColumn education_stem_degree has 0 missing values\nColumn labor_force_participation has 0 missing values\nColumn unemployment_rate has 0 missing values\nColumn self_employed has 5 missing values\nColumn farmer has 5 missing values\nColumn race_white has 0 missing values\nColumn race_black has 0 missing values\nColumn race_asian has 0 missing values\nColumn race_native has 0 missing values\nColumn race_pacific has 0 missing values\nColumn race_other has 0 missing values\nColumn race_multiple has 0 missing values\nColumn hispanic has 0 missing values\nColumn disabled has 0 missing values\nColumn poverty has 5 missing values\nColumn limited_english has 5 missing values\nColumn commute_time has 0 missing values\nColumn health_uninsured has 0 missing values\nColumn veteran has 0 missing values\nColumn Average of Jan-13 has 33 missing values\nColumn Average of Feb-13 has 3 missing values\nColumn Average of Mar-13 has 0 missing values\nColumn Average of Apr-13 has 0 missing values\nColumn Average of May-13 has 3 missing values\nColumn Average of Jun-13 has 20 missing values\nColumn Average of Jul-13 has 0 missing values\nColumn Average of Aug-13 has 17 missing values\nColumn Average of Sep-13 has 27 missing values\nColumn Average of Oct-13 has 59 missing values\nColumn Average of Nov-13 has 3 missing values\nColumn Average of Dec-13 has 3 missing values\nColumn Average of Jan-14 has 4 missing values\nColumn Average of Feb-14 has 9 missing values\nColumn Average of Mar-14 has 29 missing values\nColumn Average of Apr-14 has 180 missing values\nColumn Average of May-14 has 0 missing values\nColumn Average of Jun-14 has 152 missing values\nColumn Average of Jul-14 has 0 missing values\nColumn Average of Aug-14 has 0 missing values\nColumn Average of Sep-14 has 0 missing values\nColumn Average of Oct-14 has 0 missing values\nColumn Average of Nov-14 has 24 missing values\nColumn Average of Dec-14 has 0 missing values\nColumn Average of Jan-15 has 6 missing values\nColumn Average of Feb-15 has 12 missing values\nColumn Average of Mar-15 has 12 missing values\nColumn Average of Apr-15 has 28 missing values\nColumn Average of May-15 has 0 missing values\nColumn Average of Jun-15 has 0 missing values\nColumn Average of Jul-15 has 0 missing values\nColumn Average of Aug-15 has 22 missing values\nColumn Average of Sep-15 has 0 missing values\nColumn Average of Oct-15 has 16 missing values\nColumn Average of Nov-15 has 16 missing values\nColumn Average of Dec-15 has 18 missing values\nColumn Average of Jan-16 has 16 missing values\nColumn Average of Feb-16 has 16 missing values\nColumn Average of Mar-16 has 0 missing values\nColumn Average of Apr-16 has 0 missing values\nColumn Average of May-16 has 19 missing values\nColumn Average of Jun-16 has 0 missing values\nColumn Average of Jul-16 has 16 missing values\nColumn Average of Aug-16 has 0 missing values\nColumn Average of Sep-16 has 0 missing values\nColumn Average of Oct-16 has 0 missing values\nColumn Average of Nov-16 has 3 missing values\nColumn Average of Dec-16 has 13 missing values\nColumn Average of Jan-17 has 9 missing values\nColumn Average of Feb-17 has 0 missing values\nColumn Average of Mar-17 has 0 missing values\nColumn Average of Apr-17 has 0 missing values\nColumn Average of May-17 has 0 missing values\nColumn Average of Jun-17 has 1 missing values\nColumn Average of Jul-17 has 31 missing values\nColumn Average of Aug-17 has 0 missing values\nColumn Average of Sep-17 has 10 missing values\nColumn Average of Oct-17 has 21 missing values\nColumn Average of Nov-17 has 5 missing values\nColumn Average of Dec-17 has 0 missing values\nColumn Average of Jan-18 has 0 missing values\nColumn Average of Feb-18 has 5 missing values\nColumn Average of Mar-18 has 6 missing values\nColumn Average of Apr-18 has 0 missing values\nColumn Average of May-18 has 0 missing values\nColumn Average of Jun-18 has 9 missing values\nColumn Average of Jul-18 has 46 missing values\nColumn Average of Aug-18 has 16 missing values\nColumn Average of Sep-18 has 7 missing values\nColumn Average of Oct-18 has 7 missing values\nColumn Average of Nov-18 has 12 missing values\nColumn Average of Dec-18 has 33 missing values\nColumn metastatic_diagnosis_period has 0 missing values\n","output_type":"stream"}]},{"cell_type":"code","source":"#Repalce a few more missing values\nreplacement_value = {'home_ownership':0,\n                     'home_value':0,\n                     'rent_median':0,\n                     'rent_burden':0,\n                     'self_employed':0,\n                     'farmer':0,\n                     'poverty':0,\n                     'limited_english':0, \n                     'Average of Jan-14':0,\n                     'Average of Jan-13':0, \n                     'Average of Jan-15':0, \n                     'Average of Jul-16':0, \n                     'Average of Jan-17':0, \n                     'Average of Jul-18':0\n                    }\ntraindf.fillna(value=replacement_value, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:01:10.459858Z","iopub.execute_input":"2024-05-31T23:01:10.460236Z","iopub.status.idle":"2024-05-31T23:01:10.475678Z","shell.execute_reply.started":"2024-05-31T23:01:10.460206Z","shell.execute_reply":"2024-05-31T23:01:10.474587Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Check out missing test again\nmissing_values = testdf.isnull().sum()\nmissing_values_df = missing_values.reset_index()\nmissing_values_df.columns = ['Column', 'Missing Values']\npd.set_option('display.max_columns', None)\nfor column, count in missing_values.items():\n    print(f\"Column {column} has {count} missing values\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:18:18.460415Z","iopub.execute_input":"2024-05-29T16:18:18.460853Z","iopub.status.idle":"2024-05-29T16:18:18.473313Z","shell.execute_reply.started":"2024-05-29T16:18:18.460820Z","shell.execute_reply":"2024-05-29T16:18:18.471884Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Column patient_id has 0 missing values\nColumn patient_race has 0 missing values\nColumn payer_type has 0 missing values\nColumn patient_zip3 has 0 missing values\nColumn patient_age has 0 missing values\nColumn population has 0 missing values\nColumn density has 0 missing values\nColumn age_median has 0 missing values\nColumn age_under_10 has 0 missing values\nColumn age_10_to_19 has 0 missing values\nColumn age_20s has 0 missing values\nColumn age_30s has 0 missing values\nColumn age_40s has 0 missing values\nColumn age_50s has 0 missing values\nColumn age_60s has 0 missing values\nColumn age_70s has 0 missing values\nColumn age_over_80 has 0 missing values\nColumn male has 0 missing values\nColumn female has 0 missing values\nColumn married has 0 missing values\nColumn divorced has 0 missing values\nColumn never_married has 0 missing values\nColumn widowed has 0 missing values\nColumn income_individual_median has 0 missing values\nColumn home_ownership has 0 missing values\nColumn housing_units has 0 missing values\nColumn home_value has 0 missing values\nColumn rent_median has 0 missing values\nColumn rent_burden has 0 missing values\nColumn education_less_highschool has 0 missing values\nColumn education_highschool has 0 missing values\nColumn education_some_college has 0 missing values\nColumn education_bachelors has 0 missing values\nColumn education_graduate has 0 missing values\nColumn education_college_or_above has 0 missing values\nColumn education_stem_degree has 0 missing values\nColumn labor_force_participation has 0 missing values\nColumn unemployment_rate has 0 missing values\nColumn self_employed has 0 missing values\nColumn farmer has 0 missing values\nColumn race_white has 0 missing values\nColumn race_black has 0 missing values\nColumn race_asian has 0 missing values\nColumn race_native has 0 missing values\nColumn race_pacific has 0 missing values\nColumn race_other has 0 missing values\nColumn race_multiple has 0 missing values\nColumn hispanic has 0 missing values\nColumn disabled has 0 missing values\nColumn poverty has 0 missing values\nColumn limited_english has 0 missing values\nColumn commute_time has 0 missing values\nColumn health_uninsured has 0 missing values\nColumn veteran has 0 missing values\nColumn Average of Jan-13 has 0 missing values\nColumn Average of Feb-13 has 0 missing values\nColumn Average of Jul-13 has 0 missing values\nColumn Average of Jan-14 has 0 missing values\nColumn Average of Jul-14 has 0 missing values\nColumn Average of Jan-15 has 0 missing values\nColumn Average of Jul-15 has 0 missing values\nColumn Average of Jan-17 has 0 missing values\nColumn Average of Jan-18 has 0 missing values\nColumn Average of Jul-18 has 0 missing values\nColumn patient_race_encoded has 0 missing values\nColumn payer_type_encoded has 0 missing values\n","output_type":"stream"}]},{"cell_type":"code","source":"#Repalce a few more missing values\nreplacement_value = {'home_ownership':0,\n                     'home_value':0,\n                     'rent_median':0,\n                     'rent_burden':0,\n                     'self_employed':0,\n                     'farmer':0,\n                     'poverty':0,\n                     'limited_english':0, \n                     'Average of Jan-14':0,\n                     'Average of Jan-13':0, \n                     'Average of Jan-15':0, \n                     'Average of Jul-16':0, \n                     'Average of Jan-17':0, \n                     'Average of Jul-18':0\n                    }\ntestdf.fillna(value=replacement_value, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:01:22.490455Z","iopub.execute_input":"2024-05-31T23:01:22.490885Z","iopub.status.idle":"2024-05-31T23:01:22.506135Z","shell.execute_reply.started":"2024-05-31T23:01:22.490853Z","shell.execute_reply":"2024-05-31T23:01:22.505001Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"num_rows_shape = testdf.shape[0]\nprint(\"Number of rows in DataFrame (using shape):\", num_rows_shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:37:08.990147Z","iopub.execute_input":"2024-05-29T16:37:08.990559Z","iopub.status.idle":"2024-05-29T16:37:08.996670Z","shell.execute_reply.started":"2024-05-29T16:37:08.990524Z","shell.execute_reply":"2024-05-29T16:37:08.995455Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"Number of rows in DataFrame (using shape): 5646\n","output_type":"stream"}]},{"cell_type":"code","source":"traindf.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T15:50:14.247085Z","iopub.execute_input":"2024-05-29T15:50:14.247459Z","iopub.status.idle":"2024-05-29T15:50:14.265362Z","shell.execute_reply.started":"2024-05-29T15:50:14.247429Z","shell.execute_reply":"2024-05-29T15:50:14.264378Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 13068 entries, 0 to 13172\nData columns (total 67 columns):\n #   Column                       Non-Null Count  Dtype  \n---  ------                       --------------  -----  \n 0   patient_id                   13068 non-null  int64  \n 1   patient_race                 13068 non-null  object \n 2   payer_type                   13068 non-null  object \n 3   patient_zip3                 13068 non-null  int64  \n 4   patient_age                  13068 non-null  int64  \n 5   population                   13068 non-null  float64\n 6   density                      13068 non-null  float64\n 7   age_median                   13068 non-null  float64\n 8   age_under_10                 13068 non-null  float64\n 9   age_10_to_19                 13068 non-null  float64\n 10  age_20s                      13068 non-null  float64\n 11  age_30s                      13068 non-null  float64\n 12  age_40s                      13068 non-null  float64\n 13  age_50s                      13068 non-null  float64\n 14  age_60s                      13068 non-null  float64\n 15  age_70s                      13068 non-null  float64\n 16  age_over_80                  13068 non-null  float64\n 17  male                         13068 non-null  float64\n 18  female                       13068 non-null  float64\n 19  married                      13068 non-null  float64\n 20  divorced                     13068 non-null  float64\n 21  never_married                13068 non-null  float64\n 22  widowed                      13068 non-null  float64\n 23  income_individual_median     13068 non-null  float64\n 24  home_ownership               13068 non-null  float64\n 25  housing_units                13068 non-null  float64\n 26  home_value                   13068 non-null  float64\n 27  rent_median                  13068 non-null  float64\n 28  rent_burden                  13068 non-null  float64\n 29  education_less_highschool    13068 non-null  float64\n 30  education_highschool         13068 non-null  float64\n 31  education_some_college       13068 non-null  float64\n 32  education_bachelors          13068 non-null  float64\n 33  education_graduate           13068 non-null  float64\n 34  education_college_or_above   13068 non-null  float64\n 35  education_stem_degree        13068 non-null  float64\n 36  labor_force_participation    13068 non-null  float64\n 37  unemployment_rate            13068 non-null  float64\n 38  self_employed                13068 non-null  float64\n 39  farmer                       13068 non-null  float64\n 40  race_white                   13068 non-null  float64\n 41  race_black                   13068 non-null  float64\n 42  race_asian                   13068 non-null  float64\n 43  race_native                  13068 non-null  float64\n 44  race_pacific                 13068 non-null  float64\n 45  race_other                   13068 non-null  float64\n 46  race_multiple                13068 non-null  float64\n 47  hispanic                     13068 non-null  float64\n 48  disabled                     13068 non-null  float64\n 49  poverty                      13068 non-null  float64\n 50  limited_english              13068 non-null  float64\n 51  commute_time                 13068 non-null  float64\n 52  health_uninsured             13068 non-null  float64\n 53  veteran                      13068 non-null  float64\n 54  Average of Jan-13            13068 non-null  float64\n 55  Average of Feb-13            13068 non-null  float64\n 56  Average of Jul-13            13068 non-null  float64\n 57  Average of Jan-14            13068 non-null  float64\n 58  Average of Jul-14            13068 non-null  float64\n 59  Average of Jan-15            13068 non-null  float64\n 60  Average of Jul-15            13068 non-null  float64\n 61  Average of Jun-16            13068 non-null  float64\n 62  Average of Jan-17            13068 non-null  float64\n 63  Average of Jul-17            13068 non-null  float64\n 64  Average of Jan-18            13068 non-null  float64\n 65  Average of Jul-18            13068 non-null  float64\n 66  metastatic_diagnosis_period  13068 non-null  int64  \ndtypes: float64(61), int64(4), object(2)\nmemory usage: 6.8+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"testdf.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T15:51:54.554821Z","iopub.execute_input":"2024-05-29T15:51:54.555197Z","iopub.status.idle":"2024-05-29T15:51:54.573858Z","shell.execute_reply.started":"2024-05-29T15:51:54.555169Z","shell.execute_reply":"2024-05-29T15:51:54.572699Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 5610 entries, 0 to 5645\nData columns (total 67 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   patient_id                  5610 non-null   int64  \n 1   patient_race                5610 non-null   object \n 2   payer_type                  5610 non-null   object \n 3   patient_zip3                5610 non-null   int64  \n 4   patient_age                 5610 non-null   int64  \n 5   population                  5610 non-null   float64\n 6   density                     5610 non-null   float64\n 7   age_median                  5610 non-null   float64\n 8   age_under_10                5610 non-null   float64\n 9   age_10_to_19                5610 non-null   float64\n 10  age_20s                     5610 non-null   float64\n 11  age_30s                     5610 non-null   float64\n 12  age_40s                     5610 non-null   float64\n 13  age_50s                     5610 non-null   float64\n 14  age_60s                     5610 non-null   float64\n 15  age_70s                     5610 non-null   float64\n 16  age_over_80                 5610 non-null   float64\n 17  male                        5610 non-null   float64\n 18  female                      5610 non-null   float64\n 19  married                     5610 non-null   float64\n 20  divorced                    5610 non-null   float64\n 21  never_married               5610 non-null   float64\n 22  widowed                     5610 non-null   float64\n 23  income_individual_median    5610 non-null   float64\n 24  home_ownership              5610 non-null   float64\n 25  housing_units               5610 non-null   float64\n 26  home_value                  5610 non-null   float64\n 27  rent_median                 5610 non-null   float64\n 28  rent_burden                 5610 non-null   float64\n 29  education_less_highschool   5610 non-null   float64\n 30  education_highschool        5610 non-null   float64\n 31  education_some_college      5610 non-null   float64\n 32  education_bachelors         5610 non-null   float64\n 33  education_graduate          5610 non-null   float64\n 34  education_college_or_above  5610 non-null   float64\n 35  education_stem_degree       5610 non-null   float64\n 36  labor_force_participation   5610 non-null   float64\n 37  unemployment_rate           5610 non-null   float64\n 38  self_employed               5610 non-null   float64\n 39  farmer                      5610 non-null   float64\n 40  race_white                  5610 non-null   float64\n 41  race_black                  5610 non-null   float64\n 42  race_asian                  5610 non-null   float64\n 43  race_native                 5610 non-null   float64\n 44  race_pacific                5610 non-null   float64\n 45  race_other                  5610 non-null   float64\n 46  race_multiple               5610 non-null   float64\n 47  hispanic                    5610 non-null   float64\n 48  disabled                    5610 non-null   float64\n 49  poverty                     5610 non-null   float64\n 50  limited_english             5610 non-null   float64\n 51  commute_time                5610 non-null   float64\n 52  health_uninsured            5610 non-null   float64\n 53  veteran                     5610 non-null   float64\n 54  Average of Jan-13           5610 non-null   float64\n 55  Average of Feb-13           5610 non-null   float64\n 56  Average of Jul-13           5610 non-null   float64\n 57  Average of Jan-14           5610 non-null   float64\n 58  Average of Jul-14           5610 non-null   float64\n 59  Average of Jan-15           5610 non-null   float64\n 60  Average of Jul-15           5610 non-null   float64\n 61  Average of Jul-16           5610 non-null   float64\n 62  Average of Jan-17           5610 non-null   float64\n 63  Average of May-17           5610 non-null   float64\n 64  Average of Jan-18           5610 non-null   float64\n 65  Average of May-18           5610 non-null   float64\n 66  Average of Jul-18           5610 non-null   float64\ndtypes: float64(62), int64(3), object(2)\nmemory usage: 2.9+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"num_rows_shape = testdf.shape[0]\nprint(\"Number of rows in DataFrame (using shape):\", num_rows_shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:24:44.944460Z","iopub.execute_input":"2024-05-29T16:24:44.945398Z","iopub.status.idle":"2024-05-29T16:24:44.952507Z","shell.execute_reply.started":"2024-05-29T16:24:44.945360Z","shell.execute_reply":"2024-05-29T16:24:44.951196Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Number of rows in DataFrame (using shape): 5646\n","output_type":"stream"}]},{"cell_type":"code","source":"paydf_encoded = pd.get_dummies(traindf['payer_type'], prefix='payer_type')\npaydf_encodedt = pd.get_dummies(testdf['payer_type'], prefix='payer_type')","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:01:44.114277Z","iopub.execute_input":"2024-05-31T23:01:44.114680Z","iopub.status.idle":"2024-05-31T23:01:44.127866Z","shell.execute_reply.started":"2024-05-31T23:01:44.114643Z","shell.execute_reply":"2024-05-31T23:01:44.126840Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"one_hot_encoded = pd.get_dummies(traindf['patient_race'])\n\n# Concatenate one-hot encoded columns with original DataFrame\ntraindf_encoded = pd.concat([traindf, one_hot_encoded], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:01:49.675192Z","iopub.execute_input":"2024-05-31T23:01:49.675544Z","iopub.status.idle":"2024-05-31T23:01:49.692713Z","shell.execute_reply.started":"2024-05-31T23:01:49.675516Z","shell.execute_reply":"2024-05-31T23:01:49.691361Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"one_hot_encoded = pd.get_dummies(testdf['patient_race'])\n\n# Concatenate one-hot encoded columns with original DataFrame\ntestdf_encoded = pd.concat([testdf, one_hot_encoded], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:02:11.009916Z","iopub.execute_input":"2024-05-31T23:02:11.010282Z","iopub.status.idle":"2024-05-31T23:02:11.020058Z","shell.execute_reply.started":"2024-05-31T23:02:11.010255Z","shell.execute_reply":"2024-05-31T23:02:11.018867Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"traindf['patient_race_encoded'] = traindf['patient_race'].astype('category').cat.codes","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:02:16.345431Z","iopub.execute_input":"2024-05-31T23:02:16.346299Z","iopub.status.idle":"2024-05-31T23:02:16.355075Z","shell.execute_reply.started":"2024-05-31T23:02:16.346257Z","shell.execute_reply":"2024-05-31T23:02:16.353690Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"testdf['patient_race_encoded'] = testdf['patient_race'].astype('category').cat.codes","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:02:35.290108Z","iopub.execute_input":"2024-05-31T23:02:35.290493Z","iopub.status.idle":"2024-05-31T23:02:35.297894Z","shell.execute_reply.started":"2024-05-31T23:02:35.290461Z","shell.execute_reply":"2024-05-31T23:02:35.296361Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(f\"Unique values for patient_race_encoded:\")\nprint(traindf['patient_race_encoded'].unique())\nprint()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:37:41.763531Z","iopub.execute_input":"2024-05-29T16:37:41.763978Z","iopub.status.idle":"2024-05-29T16:37:41.771561Z","shell.execute_reply.started":"2024-05-29T16:37:41.763944Z","shell.execute_reply":"2024-05-29T16:37:41.770724Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"Unique values for patient_race_encoded:\n[4 5 2 1 3 0]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Unique values for patient_race_encoded:\")\nprint(testdf['patient_race_encoded'].unique())\nprint()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:37:44.414946Z","iopub.execute_input":"2024-05-29T16:37:44.415323Z","iopub.status.idle":"2024-05-29T16:37:44.422479Z","shell.execute_reply.started":"2024-05-29T16:37:44.415292Z","shell.execute_reply":"2024-05-29T16:37:44.420991Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"Unique values for patient_race_encoded:\n[4 1 0 5 2 3]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"traindf['payer_type_encoded'] = traindf['payer_type'].astype('category').cat.codes\ntestdf['payer_type_encoded'] = testdf['payer_type'].astype('category').cat.codes","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:02:42.733090Z","iopub.execute_input":"2024-05-31T23:02:42.733461Z","iopub.status.idle":"2024-05-31T23:02:42.743812Z","shell.execute_reply.started":"2024-05-31T23:02:42.733432Z","shell.execute_reply":"2024-05-31T23:02:42.742550Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(f\"Unique values for payer_type_encoded:\")\nprint(traindf['payer_type_encoded'].unique())\nprint()\n\nprint(f\"Unique values for payer_type_encoded:\")\nprint(testdf['payer_type_encoded'].unique())\nprint()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:37:51.289559Z","iopub.execute_input":"2024-05-29T16:37:51.289947Z","iopub.status.idle":"2024-05-29T16:37:51.298802Z","shell.execute_reply.started":"2024-05-29T16:37:51.289917Z","shell.execute_reply":"2024-05-29T16:37:51.296974Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"Unique values for payer_type_encoded:\n[0 3 1 2]\n\nUnique values for payer_type_encoded:\n[0 3 2 1]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"num_rows_shape = testdf.shape[0]\nprint(\"Number of rows in DataFrame (using shape):\", num_rows_shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:02:54.197432Z","iopub.execute_input":"2024-05-31T23:02:54.198433Z","iopub.status.idle":"2024-05-31T23:02:54.204156Z","shell.execute_reply.started":"2024-05-31T23:02:54.198393Z","shell.execute_reply":"2024-05-31T23:02:54.202956Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Number of rows in DataFrame (using shape): 5646\n","output_type":"stream"}]},{"cell_type":"code","source":"#Check out missing again\nmissing_values = traindf.isnull().sum()\nmissing_values_df = missing_values.reset_index()\nmissing_values_df.columns = ['Column', 'Missing Values']\npd.set_option('display.max_columns', None)\nfor column, count in missing_values.items():\n    print(f\"Column {column} has {count} missing values\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T15:55:50.417333Z","iopub.execute_input":"2024-05-29T15:55:50.417849Z","iopub.status.idle":"2024-05-29T15:55:50.436365Z","shell.execute_reply.started":"2024-05-29T15:55:50.417806Z","shell.execute_reply":"2024-05-29T15:55:50.435205Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Column patient_id has 0 missing values\nColumn patient_race has 0 missing values\nColumn payer_type has 0 missing values\nColumn patient_zip3 has 0 missing values\nColumn patient_age has 0 missing values\nColumn population has 0 missing values\nColumn density has 0 missing values\nColumn age_median has 0 missing values\nColumn age_under_10 has 0 missing values\nColumn age_10_to_19 has 0 missing values\nColumn age_20s has 0 missing values\nColumn age_30s has 0 missing values\nColumn age_40s has 0 missing values\nColumn age_50s has 0 missing values\nColumn age_60s has 0 missing values\nColumn age_70s has 0 missing values\nColumn age_over_80 has 0 missing values\nColumn male has 0 missing values\nColumn female has 0 missing values\nColumn married has 0 missing values\nColumn divorced has 0 missing values\nColumn never_married has 0 missing values\nColumn widowed has 0 missing values\nColumn income_individual_median has 0 missing values\nColumn home_ownership has 0 missing values\nColumn housing_units has 0 missing values\nColumn home_value has 0 missing values\nColumn rent_median has 0 missing values\nColumn rent_burden has 0 missing values\nColumn education_less_highschool has 0 missing values\nColumn education_highschool has 0 missing values\nColumn education_some_college has 0 missing values\nColumn education_bachelors has 0 missing values\nColumn education_graduate has 0 missing values\nColumn education_college_or_above has 0 missing values\nColumn education_stem_degree has 0 missing values\nColumn labor_force_participation has 0 missing values\nColumn unemployment_rate has 0 missing values\nColumn self_employed has 0 missing values\nColumn farmer has 0 missing values\nColumn race_white has 0 missing values\nColumn race_black has 0 missing values\nColumn race_asian has 0 missing values\nColumn race_native has 0 missing values\nColumn race_pacific has 0 missing values\nColumn race_other has 0 missing values\nColumn race_multiple has 0 missing values\nColumn hispanic has 0 missing values\nColumn disabled has 0 missing values\nColumn poverty has 0 missing values\nColumn limited_english has 0 missing values\nColumn commute_time has 0 missing values\nColumn health_uninsured has 0 missing values\nColumn veteran has 0 missing values\nColumn Average of Jan-13 has 0 missing values\nColumn Average of Feb-13 has 0 missing values\nColumn Average of Jul-13 has 0 missing values\nColumn Average of Jan-14 has 0 missing values\nColumn Average of Jul-14 has 0 missing values\nColumn Average of Jan-15 has 0 missing values\nColumn Average of Jul-15 has 0 missing values\nColumn Average of Jun-16 has 0 missing values\nColumn Average of Jan-17 has 0 missing values\nColumn Average of Jul-17 has 0 missing values\nColumn Average of Jan-18 has 0 missing values\nColumn Average of Jul-18 has 0 missing values\nColumn metastatic_diagnosis_period has 0 missing values\nColumn patient_race_encoded has 0 missing values\nColumn payer_type_encoded has 0 missing values\n","output_type":"stream"}]},{"cell_type":"code","source":"#Check out missing again\nmissing_values = testdf.isnull().sum()\nmissing_values_df = missing_values.reset_index()\nmissing_values_df.columns = ['Column', 'Missing Values']\npd.set_option('display.max_columns', None)\nfor column, count in missing_values.items():\n    print(f\"Column {column} has {count} missing values\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T15:56:11.721146Z","iopub.execute_input":"2024-05-29T15:56:11.721511Z","iopub.status.idle":"2024-05-29T15:56:11.731375Z","shell.execute_reply.started":"2024-05-29T15:56:11.721484Z","shell.execute_reply":"2024-05-29T15:56:11.730477Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Column patient_id has 0 missing values\nColumn patient_race has 0 missing values\nColumn payer_type has 0 missing values\nColumn patient_zip3 has 0 missing values\nColumn patient_age has 0 missing values\nColumn population has 0 missing values\nColumn density has 0 missing values\nColumn age_median has 0 missing values\nColumn age_under_10 has 0 missing values\nColumn age_10_to_19 has 0 missing values\nColumn age_20s has 0 missing values\nColumn age_30s has 0 missing values\nColumn age_40s has 0 missing values\nColumn age_50s has 0 missing values\nColumn age_60s has 0 missing values\nColumn age_70s has 0 missing values\nColumn age_over_80 has 0 missing values\nColumn male has 0 missing values\nColumn female has 0 missing values\nColumn married has 0 missing values\nColumn divorced has 0 missing values\nColumn never_married has 0 missing values\nColumn widowed has 0 missing values\nColumn income_individual_median has 0 missing values\nColumn home_ownership has 0 missing values\nColumn housing_units has 0 missing values\nColumn home_value has 0 missing values\nColumn rent_median has 0 missing values\nColumn rent_burden has 0 missing values\nColumn education_less_highschool has 0 missing values\nColumn education_highschool has 0 missing values\nColumn education_some_college has 0 missing values\nColumn education_bachelors has 0 missing values\nColumn education_graduate has 0 missing values\nColumn education_college_or_above has 0 missing values\nColumn education_stem_degree has 0 missing values\nColumn labor_force_participation has 0 missing values\nColumn unemployment_rate has 0 missing values\nColumn self_employed has 0 missing values\nColumn farmer has 0 missing values\nColumn race_white has 0 missing values\nColumn race_black has 0 missing values\nColumn race_asian has 0 missing values\nColumn race_native has 0 missing values\nColumn race_pacific has 0 missing values\nColumn race_other has 0 missing values\nColumn race_multiple has 0 missing values\nColumn hispanic has 0 missing values\nColumn disabled has 0 missing values\nColumn poverty has 0 missing values\nColumn limited_english has 0 missing values\nColumn commute_time has 0 missing values\nColumn health_uninsured has 0 missing values\nColumn veteran has 0 missing values\nColumn Average of Jan-13 has 0 missing values\nColumn Average of Feb-13 has 0 missing values\nColumn Average of Jul-13 has 0 missing values\nColumn Average of Jan-14 has 0 missing values\nColumn Average of Jul-14 has 0 missing values\nColumn Average of Jan-15 has 0 missing values\nColumn Average of Jul-15 has 0 missing values\nColumn Average of Jul-16 has 0 missing values\nColumn Average of Jan-17 has 0 missing values\nColumn Average of May-17 has 0 missing values\nColumn Average of Jan-18 has 0 missing values\nColumn Average of May-18 has 0 missing values\nColumn Average of Jul-18 has 0 missing values\nColumn patient_race_encoded has 0 missing values\nColumn payer_type_encoded has 0 missing values\n","output_type":"stream"}]},{"cell_type":"code","source":"traindf.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T15:56:38.583016Z","iopub.execute_input":"2024-05-29T15:56:38.583449Z","iopub.status.idle":"2024-05-29T15:56:38.600683Z","shell.execute_reply.started":"2024-05-29T15:56:38.583413Z","shell.execute_reply":"2024-05-29T15:56:38.599120Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 13068 entries, 0 to 13172\nData columns (total 69 columns):\n #   Column                       Non-Null Count  Dtype  \n---  ------                       --------------  -----  \n 0   patient_id                   13068 non-null  int64  \n 1   patient_race                 13068 non-null  object \n 2   payer_type                   13068 non-null  object \n 3   patient_zip3                 13068 non-null  int64  \n 4   patient_age                  13068 non-null  int64  \n 5   population                   13068 non-null  float64\n 6   density                      13068 non-null  float64\n 7   age_median                   13068 non-null  float64\n 8   age_under_10                 13068 non-null  float64\n 9   age_10_to_19                 13068 non-null  float64\n 10  age_20s                      13068 non-null  float64\n 11  age_30s                      13068 non-null  float64\n 12  age_40s                      13068 non-null  float64\n 13  age_50s                      13068 non-null  float64\n 14  age_60s                      13068 non-null  float64\n 15  age_70s                      13068 non-null  float64\n 16  age_over_80                  13068 non-null  float64\n 17  male                         13068 non-null  float64\n 18  female                       13068 non-null  float64\n 19  married                      13068 non-null  float64\n 20  divorced                     13068 non-null  float64\n 21  never_married                13068 non-null  float64\n 22  widowed                      13068 non-null  float64\n 23  income_individual_median     13068 non-null  float64\n 24  home_ownership               13068 non-null  float64\n 25  housing_units                13068 non-null  float64\n 26  home_value                   13068 non-null  float64\n 27  rent_median                  13068 non-null  float64\n 28  rent_burden                  13068 non-null  float64\n 29  education_less_highschool    13068 non-null  float64\n 30  education_highschool         13068 non-null  float64\n 31  education_some_college       13068 non-null  float64\n 32  education_bachelors          13068 non-null  float64\n 33  education_graduate           13068 non-null  float64\n 34  education_college_or_above   13068 non-null  float64\n 35  education_stem_degree        13068 non-null  float64\n 36  labor_force_participation    13068 non-null  float64\n 37  unemployment_rate            13068 non-null  float64\n 38  self_employed                13068 non-null  float64\n 39  farmer                       13068 non-null  float64\n 40  race_white                   13068 non-null  float64\n 41  race_black                   13068 non-null  float64\n 42  race_asian                   13068 non-null  float64\n 43  race_native                  13068 non-null  float64\n 44  race_pacific                 13068 non-null  float64\n 45  race_other                   13068 non-null  float64\n 46  race_multiple                13068 non-null  float64\n 47  hispanic                     13068 non-null  float64\n 48  disabled                     13068 non-null  float64\n 49  poverty                      13068 non-null  float64\n 50  limited_english              13068 non-null  float64\n 51  commute_time                 13068 non-null  float64\n 52  health_uninsured             13068 non-null  float64\n 53  veteran                      13068 non-null  float64\n 54  Average of Jan-13            13068 non-null  float64\n 55  Average of Feb-13            13068 non-null  float64\n 56  Average of Jul-13            13068 non-null  float64\n 57  Average of Jan-14            13068 non-null  float64\n 58  Average of Jul-14            13068 non-null  float64\n 59  Average of Jan-15            13068 non-null  float64\n 60  Average of Jul-15            13068 non-null  float64\n 61  Average of Jun-16            13068 non-null  float64\n 62  Average of Jan-17            13068 non-null  float64\n 63  Average of Jul-17            13068 non-null  float64\n 64  Average of Jan-18            13068 non-null  float64\n 65  Average of Jul-18            13068 non-null  float64\n 66  metastatic_diagnosis_period  13068 non-null  int64  \n 67  patient_race_encoded         13068 non-null  int8   \n 68  payer_type_encoded           13068 non-null  int8   \ndtypes: float64(61), int64(4), int8(2), object(2)\nmemory usage: 6.8+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"testdf.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T15:56:57.746038Z","iopub.execute_input":"2024-05-29T15:56:57.746388Z","iopub.status.idle":"2024-05-29T15:56:57.761119Z","shell.execute_reply.started":"2024-05-29T15:56:57.746360Z","shell.execute_reply":"2024-05-29T15:56:57.760016Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 5610 entries, 0 to 5645\nData columns (total 69 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   patient_id                  5610 non-null   int64  \n 1   patient_race                5610 non-null   object \n 2   payer_type                  5610 non-null   object \n 3   patient_zip3                5610 non-null   int64  \n 4   patient_age                 5610 non-null   int64  \n 5   population                  5610 non-null   float64\n 6   density                     5610 non-null   float64\n 7   age_median                  5610 non-null   float64\n 8   age_under_10                5610 non-null   float64\n 9   age_10_to_19                5610 non-null   float64\n 10  age_20s                     5610 non-null   float64\n 11  age_30s                     5610 non-null   float64\n 12  age_40s                     5610 non-null   float64\n 13  age_50s                     5610 non-null   float64\n 14  age_60s                     5610 non-null   float64\n 15  age_70s                     5610 non-null   float64\n 16  age_over_80                 5610 non-null   float64\n 17  male                        5610 non-null   float64\n 18  female                      5610 non-null   float64\n 19  married                     5610 non-null   float64\n 20  divorced                    5610 non-null   float64\n 21  never_married               5610 non-null   float64\n 22  widowed                     5610 non-null   float64\n 23  income_individual_median    5610 non-null   float64\n 24  home_ownership              5610 non-null   float64\n 25  housing_units               5610 non-null   float64\n 26  home_value                  5610 non-null   float64\n 27  rent_median                 5610 non-null   float64\n 28  rent_burden                 5610 non-null   float64\n 29  education_less_highschool   5610 non-null   float64\n 30  education_highschool        5610 non-null   float64\n 31  education_some_college      5610 non-null   float64\n 32  education_bachelors         5610 non-null   float64\n 33  education_graduate          5610 non-null   float64\n 34  education_college_or_above  5610 non-null   float64\n 35  education_stem_degree       5610 non-null   float64\n 36  labor_force_participation   5610 non-null   float64\n 37  unemployment_rate           5610 non-null   float64\n 38  self_employed               5610 non-null   float64\n 39  farmer                      5610 non-null   float64\n 40  race_white                  5610 non-null   float64\n 41  race_black                  5610 non-null   float64\n 42  race_asian                  5610 non-null   float64\n 43  race_native                 5610 non-null   float64\n 44  race_pacific                5610 non-null   float64\n 45  race_other                  5610 non-null   float64\n 46  race_multiple               5610 non-null   float64\n 47  hispanic                    5610 non-null   float64\n 48  disabled                    5610 non-null   float64\n 49  poverty                     5610 non-null   float64\n 50  limited_english             5610 non-null   float64\n 51  commute_time                5610 non-null   float64\n 52  health_uninsured            5610 non-null   float64\n 53  veteran                     5610 non-null   float64\n 54  Average of Jan-13           5610 non-null   float64\n 55  Average of Feb-13           5610 non-null   float64\n 56  Average of Jul-13           5610 non-null   float64\n 57  Average of Jan-14           5610 non-null   float64\n 58  Average of Jul-14           5610 non-null   float64\n 59  Average of Jan-15           5610 non-null   float64\n 60  Average of Jul-15           5610 non-null   float64\n 61  Average of Jul-16           5610 non-null   float64\n 62  Average of Jan-17           5610 non-null   float64\n 63  Average of May-17           5610 non-null   float64\n 64  Average of Jan-18           5610 non-null   float64\n 65  Average of May-18           5610 non-null   float64\n 66  Average of Jul-18           5610 non-null   float64\n 67  patient_race_encoded        5610 non-null   int8   \n 68  payer_type_encoded          5610 non-null   int8   \ndtypes: float64(62), int64(3), int8(2), object(2)\nmemory usage: 2.9+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"traindf = traindf.drop(columns=['patient_race', 'payer_type'])\ntestdf = testdf.drop(columns=['patient_race', 'payer_type'])","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:03:17.351880Z","iopub.execute_input":"2024-05-31T23:03:17.352266Z","iopub.status.idle":"2024-05-31T23:03:17.368123Z","shell.execute_reply.started":"2024-05-31T23:03:17.352233Z","shell.execute_reply":"2024-05-31T23:03:17.366681Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nX = pd.DataFrame(traindf, columns=[\"patient_age\", \"patient_race_encoded\", \"payer_type_encoded\", \"limited_english\", \"health_uninsured\"])\ny = pd.DataFrame(traindf['metastatic_diagnosis_period'], columns=['metastatic_diagnosis_period'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LinearRegression()\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\n# Calculate Mean Squared Error\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\n\n# Coefficients and Intercept\nprint(\"Coefficients:\", model.coef_)\nprint(\"Intercept:\", model.intercept_)\nrmse = np.sqrt(mse)\nprint (rmse)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:03:24.988127Z","iopub.execute_input":"2024-05-31T23:03:24.988517Z","iopub.status.idle":"2024-05-31T23:03:25.741153Z","shell.execute_reply.started":"2024-05-31T23:03:24.988485Z","shell.execute_reply":"2024-05-31T23:03:25.739241Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Mean Squared Error: 12083.812520779797\nCoefficients: [[-0.29914845 -0.25520996 -5.62865571  0.3202197  -0.16774535]]\nIntercept: [119.95145324]\n109.92639592372615\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nX = pd.DataFrame(traindf, columns=[\"patient_age\", \"patient_race_encoded\", \"payer_type_encoded\", \"limited_english\", \"health_uninsured\"])\ny = pd.DataFrame(traindf['metastatic_diagnosis_period'], columns=['metastatic_diagnosis_period'])\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a Random Forest Regressor model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Fit the model to the training data\nmodel.fit(X_train, y_train)\n\n# Predict on the test data\ny_pred = model.predict(X_test)\n\n# Calculate Mean Squared Error\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\nrmse = np.sqrt(mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:03:44.290387Z","iopub.execute_input":"2024-05-31T23:03:44.290780Z","iopub.status.idle":"2024-05-31T23:03:48.208035Z","shell.execute_reply.started":"2024-05-31T23:03:44.290749Z","shell.execute_reply":"2024-05-31T23:03:48.206662Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2081386723.py:17: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  model.fit(X_train, y_train)\n","output_type":"stream"},{"name":"stdout","text":"Mean Squared Error: 13407.821669587558\nRoot Mean Squared Error (RMSE): 115.79214856624588\n","output_type":"stream"}]},{"cell_type":"code","source":"num_rows_shape = testdf.shape[0]\nprint(\"Number of rows in DataFrame (using shape):\", num_rows_shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:03:59.219488Z","iopub.execute_input":"2024-05-31T23:03:59.219913Z","iopub.status.idle":"2024-05-31T23:03:59.226103Z","shell.execute_reply.started":"2024-05-31T23:03:59.219880Z","shell.execute_reply":"2024-05-31T23:03:59.224900Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Number of rows in DataFrame (using shape): 5646\n","output_type":"stream"}]},{"cell_type":"code","source":"X = pd.DataFrame(traindf, columns=[\"patient_age\", \"patient_race_encoded\", \"payer_type_encoded\", \"limited_english\", \"health_uninsured\"])\ny = pd.DataFrame(traindf['metastatic_diagnosis_period'], columns=['metastatic_diagnosis_period'])\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a Random Forest Regressor model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Fit the model to the training data\nmodel.fit(X_train, y_train)\n\n# Predict on the test data\ny_pred = model.predict(X_test)\n\n# Calculate Mean Squared Error\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\nrmse = np.sqrt(mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:04:07.612002Z","iopub.execute_input":"2024-05-31T23:04:07.613063Z","iopub.status.idle":"2024-05-31T23:04:11.019272Z","shell.execute_reply.started":"2024-05-31T23:04:07.613017Z","shell.execute_reply":"2024-05-31T23:04:11.018022Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3247202205.py:11: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  model.fit(X_train, y_train)\n","output_type":"stream"},{"name":"stdout","text":"Mean Squared Error: 13407.821669587558\nRoot Mean Squared Error (RMSE): 115.79214856624588\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\nX = traindf[[\"patient_age\", \"payer_type_encoded\", \"health_uninsured\",\"Average of Jul-18\"]]\ny = traindf['metastatic_diagnosis_period']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Initialize the GradientBoostingRegressor with desired parameters\ngb_regressor = GradientBoostingRegressor()\n\n# Train the model\ngb_regressor.fit(X_train, y_train)\n\n# Make predictions on the test data\npredictions = gb_regressor.predict(X_test)\n\n# Calculate Root Mean Squared Error\nrmse = np.sqrt(mean_squared_error(y_test, predictions))\n\nprint(\"Root Mean Squared Error:\", rmse)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:04:19.233050Z","iopub.execute_input":"2024-05-31T23:04:19.233427Z","iopub.status.idle":"2024-05-31T23:04:19.925659Z","shell.execute_reply.started":"2024-05-31T23:04:19.233395Z","shell.execute_reply":"2024-05-31T23:04:19.924613Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Root Mean Squared Error: 106.50040065803536\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\nX = traindf[[\"patient_age\",\"patient_race_encoded\", \"limited_english\", \"poverty\", \"payer_type_encoded\", \"health_uninsured\",\"Average of Jul-18\"]]\ny = traindf['metastatic_diagnosis_period']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Initialize the GradientBoostingRegressor with desired parameters\ngb_regressor = GradientBoostingRegressor()\n\n# Train the model\ngb_regressor.fit(X_train, y_train)\n\n# Make predictions on the test data\npredictions = gb_regressor.predict(X_test)\n\n# Calculate Root Mean Squared Error\nrmse = np.sqrt(mean_squared_error(y_test, predictions))\n\nprint(\"Root Mean Squared Error:\", rmse)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:07:17.364072Z","iopub.execute_input":"2024-05-31T23:07:17.364491Z","iopub.status.idle":"2024-05-31T23:07:18.411794Z","shell.execute_reply.started":"2024-05-31T23:07:17.364458Z","shell.execute_reply":"2024-05-31T23:07:18.410460Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Root Mean Squared Error: 106.57151411231344\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n\npatient_ids=testdf[\"patient_id\"]\nX = traindf[[\"patient_age\",\"patient_race_encoded\", \"payer_type_encoded\", \"health_uninsured\",\"Average of Jul-18\"]]\ny = traindf['metastatic_diagnosis_period']\n\nX_test = testdf[[\"patient_age\",\"patient_race_encoded\", \"payer_type_encoded\", \"health_uninsured\",\"Average of Jul-18\"]] \n\nmodel = GradientBoostingRegressor() \n\nmodel.fit(X_train, y_train)\n\npredictions=model.predict(X_test)\n\n# Create a DataFrame to store the results, including patient ID\nresults_df = pd.DataFrame({\"patient_id\": patient_ids, \"predicted_value\": predictions})\n\n# Print or output results table\nprint(results_df.head())\n\n# Save results table to CSV file\nresults_df.to_csv('predictions_with_patient_id.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:39:04.872404Z","iopub.execute_input":"2024-05-29T16:39:04.873440Z","iopub.status.idle":"2024-05-29T16:39:05.644619Z","shell.execute_reply.started":"2024-05-29T16:39:04.873392Z","shell.execute_reply":"2024-05-29T16:39:05.643747Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"   patient_id  predicted_value\n0      730681        96.746534\n1      334212        82.861337\n2      571362       110.211306\n3      907331       101.284335\n4      208382        61.758542\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# sklearn imports\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RandomizedSearchCV, GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor, VotingClassifier, StackingRegressor, GradientBoostingRegressor,  ExtraTreesRegressor\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.feature_selection import RFE, SequentialFeatureSelector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import RidgeCV, LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\n\n# catboost import\nfrom catboost import CatBoostRegressor\n\n# xgboost imports\nfrom xgboost import XGBRegressor, XGBRFRegressor\nimport xgboost as xgb\n\n# lightgbm import\nfrom lightgbm import LGBMRegressor\n\n# Set theme for seaborn\nsns.set_theme(style='white')\n\n# Define RMSE\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\n# Pandas display settings\npd.set_option('display.max_columns', None)\npd.options.display.max_rows = None\npd.set_option('display.max_colwidth', 200)\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set seed for reproducibility\nseed = 42\nplt.style.use('ggplot')\n\n# CatBoost\nprint(\"------------------CatBoost---------------\")\nctb = CatBoostRegressor(\n    random_state=seed,\n    learning_rate=.02,\n    verbose=False,\n    eval_metric='RMSE'\n).fit(X_train, y_train, eval_set=(X_test, y_test), use_best_model=True)\nprint(\"CatBoost RMSE:\", rmse(y_test, ctb.predict(X_test)))\n\n# XGBoost\nprint(\"------------------XGBoost---------------\")\nxgb = XGBRegressor(\n    random_state=seed,\n    learning_rate=.03,\n    max_depth=4\n).fit(X_train, y_train)\nprint(\"XGBoost RMSE:\", rmse(y_test, xgb.predict(X_test)))\n\n# LightGBM\nprint(\"------------------LightGBM---------------\")\nlgbm = LGBMRegressor(\n    random_state=seed,\n    learning_rate=.04,\n    verbose=0\n).fit(X_train, y_train)\nprint(\"LightGBM RMSE:\", rmse(y_test, lgbm.predict(X_test)))\n\n# Random Forest\nprint(\"------------------Random Forest---------------\")\nrf = RandomForestRegressor(\n    random_state=seed,\n    max_depth=6\n).fit(X_train, y_train)\nprint(\"Random Forest RMSE:\", rmse(y_test, rf.predict(X_test)))\n\n# Gradient Boosting\nprint(\"------------------Gradient Boosting---------------\")\ngb = GradientBoostingRegressor(\n    random_state=seed,\n    learning_rate=.05\n).fit(X_train, y_train)\nprint(\"Gradient Boosting RMSE:\", rmse(y_test, gb.predict(X_test)))\n\n# Extra Trees\nprint(\"------------------Extra Trees---------------\")\net = ExtraTreesRegressor(\n    random_state=seed,\n    max_depth=9\n).fit(X_train, y_train)\nprint(\"Extra Trees RMSE:\", rmse(y_test, et.predict(X_test)))","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:45:41.308590Z","iopub.execute_input":"2024-05-29T16:45:41.309033Z","iopub.status.idle":"2024-05-29T16:45:44.471682Z","shell.execute_reply.started":"2024-05-29T16:45:41.309001Z","shell.execute_reply":"2024-05-29T16:45:44.469676Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stdout","text":"------------------CatBoost---------------\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[112], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# CatBoost\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------CatBoost---------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m ctb \u001b[38;5;241m=\u001b[39m \u001b[43mCatBoostRegressor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.02\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRMSE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     52\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCatBoost RMSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rmse(y_test, ctb\u001b[38;5;241m.\u001b[39mpredict(X_test)))\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# XGBoost\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/catboost/core.py:5807\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m   5805\u001b[0m     CatBoostRegressor\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m-> 5807\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5808\u001b[0m \u001b[43m                 \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5809\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5810\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/catboost/core.py:2381\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, PATH_TYPES \u001b[38;5;241m+\u001b[39m (Pool,)):\n\u001b[1;32m   2379\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my may be None only when X is an instance of catboost.Pool or string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2381\u001b[0m train_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_train_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2384\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2385\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2387\u001b[0m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2388\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2391\u001b[0m params \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2392\u001b[0m train_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/catboost/core.py:2334\u001b[0m, in \u001b[0;36mCatBoost._prepare_train_params\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[0m\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_set[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m eval_set[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_set\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m tuple contains at least one None value\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2333\u001b[0m eval_sets\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m-> 2334\u001b[0m     \u001b[43mPool\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2335\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cat_feature_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_feature_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2339\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embedding_feature_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2341\u001b[0m )\n\u001b[1;32m   2343\u001b[0m eval_total_row_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m eval_sets[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnum_row()\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_sets[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnum_row() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/catboost/core.py:844\u001b[0m, in \u001b[0;36mPool.__init__\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr, data_can_be_none)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature_names, PATH_TYPES):\n\u001b[1;32m    839\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\n\u001b[1;32m    840\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_names must be None or have non-string type when the pool is created from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython objects.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m             )\n\u001b[0;32m--> 844\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_can_be_none:\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/catboost/core.py:1425\u001b[0m, in \u001b[0;36mPool._init\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\u001b[0m\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(label)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1424\u001b[0m         label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(label, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1425\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_label_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(feature_names, features_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/catboost/core.py:965\u001b[0m, in \u001b[0;36mPool._check_label_shape\u001b[0;34m(self, label, samples_count)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;124;03mCheck label length and dimension.\u001b[39;00m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(label) \u001b[38;5;241m!=\u001b[39m samples_count:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of label=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and length of data=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is different.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(label), samples_count))\n","\u001b[0;31mCatBoostError\u001b[0m: Length of label=2635 and length of data=5646 is different."],"ename":"CatBoostError","evalue":"Length of label=2635 and length of data=5646 is different.","output_type":"error"}]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n\n\nX = traindf.drop(columns=['metastatic_diagnosis_period'])  # Features\ny = traindf['metastatic_diagnosis_period']  # Target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the Random Forest Regressor with desired parameters\nrf_regressor = RandomForestRegressor()\n\n# Train the model\nrf_regressor.fit(X_train, y_train)\n\n# Make predictions on the test data\npredictions = rf_regressor.predict(X_test)\n\n# Calculate Root Mean Squared Error\nrmse = np.sqrt(mean_squared_error(y_test, predictions))\nprint(\"Root Mean Squared Error:\", rmse)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:05:19.070490Z","iopub.execute_input":"2024-05-29T16:05:19.070933Z","iopub.status.idle":"2024-05-29T16:06:03.708481Z","shell.execute_reply.started":"2024-05-29T16:05:19.070902Z","shell.execute_reply":"2024-05-29T16:06:03.707645Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Root Mean Squared Error: 111.91758732205268\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\nX = traindf.drop(columns=['metastatic_diagnosis_period'])\ny = traindf['metastatic_diagnosis_period']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the GradientBoostingRegressor with desired parameters\ngb_regressor = GradientBoostingRegressor()\n\n# Train the model\ngb_regressor.fit(X_train, y_train)\n\n# Make predictions on the test data\npredictions = gb_regressor.predict(X_test)\n\n# Calculate Root Mean Squared Error\nrmse = np.sqrt(mean_squared_error(y_test, predictions))\n\nprint(\"Root Mean Squared Error:\", rmse)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:07:20.173751Z","iopub.execute_input":"2024-05-29T16:07:20.174136Z","iopub.status.idle":"2024-05-29T16:07:28.456511Z","shell.execute_reply.started":"2024-05-29T16:07:20.174104Z","shell.execute_reply":"2024-05-29T16:07:28.455309Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Root Mean Squared Error: 105.30592885821609\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# sklearn imports\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RandomizedSearchCV, GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor, VotingClassifier, StackingRegressor, GradientBoostingRegressor,  ExtraTreesRegressor\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.feature_selection import RFE, SequentialFeatureSelector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import RidgeCV, LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\n\n# catboost import\nfrom catboost import CatBoostRegressor\n\n# xgboost imports\nfrom xgboost import XGBRegressor, XGBRFRegressor\nimport xgboost as xgb\n\n# lightgbm import\nfrom lightgbm import LGBMRegressor\n\n# Set theme for seaborn\nsns.set_theme(style='white')\n\n# Define RMSE\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\n# Pandas display settings\npd.set_option('display.max_columns', None)\npd.options.display.max_rows = None\npd.set_option('display.max_colwidth', 200)\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set seed for reproducibility\nseed = 42\nplt.style.use('ggplot')\n\n# CatBoost\nprint(\"------------------CatBoost---------------\")\nctb = CatBoostRegressor(\n    random_state=seed,\n    learning_rate=.02,\n    verbose=False,\n    eval_metric='RMSE'\n).fit(X_train, y_train, eval_set=(X_test, y_test), use_best_model=True)\nprint(\"CatBoost RMSE:\", rmse(y_test, ctb.predict(X_test)))\n\n# XGBoost\nprint(\"------------------XGBoost---------------\")\nxgb = XGBRegressor(\n    random_state=seed,\n    learning_rate=.03,\n    max_depth=4\n).fit(X_train, y_train)\nprint(\"XGBoost RMSE:\", rmse(y_test, xgb.predict(X_test)))\n\n# LightGBM\nprint(\"------------------LightGBM---------------\")\nlgbm = LGBMRegressor(\n    random_state=seed,\n    learning_rate=.04,\n    verbose=0\n).fit(X_train, y_train)\nprint(\"LightGBM RMSE:\", rmse(y_test, lgbm.predict(X_test)))\n\n# Random Forest\nprint(\"------------------Random Forest---------------\")\nrf = RandomForestRegressor(\n    random_state=seed,\n    max_depth=6\n).fit(X_train, y_train)\nprint(\"Random Forest RMSE:\", rmse(y_test, rf.predict(X_test)))\n\n# Gradient Boosting\nprint(\"------------------Gradient Boosting---------------\")\ngb = GradientBoostingRegressor(\n    random_state=seed,\n    learning_rate=.05\n).fit(X_train, y_train)\nprint(\"Gradient Boosting RMSE:\", rmse(y_test, gb.predict(X_test)))\n\n# Extra Trees\nprint(\"------------------Extra Trees---------------\")\net = ExtraTreesRegressor(\n    random_state=seed,\n    max_depth=9\n).fit(X_train, y_train)\nprint(\"Extra Trees RMSE:\", rmse(y_test, et.predict(X_test)))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:57:54.364009Z","iopub.execute_input":"2024-05-27T19:57:54.364905Z","iopub.status.idle":"2024-05-27T19:58:43.418509Z","shell.execute_reply.started":"2024-05-27T19:57:54.364847Z","shell.execute_reply":"2024-05-27T19:58:43.417239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame to store the predictions along with patient_id\npatient_id = traindf['patient_id']\noutput_df = pd.DataFrame({\n    'patient_id':patient_id[:2611],  \n    'metastatic_diagnosis_period': y_pred\n})\n\n# Save the predictions to a new CSV file\noutput_df.to_csv('submission.csv', index=False)\n\nprint(\"Predictions saved to test_predictions.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T20:07:41.110481Z","iopub.execute_input":"2024-05-27T20:07:41.111002Z","iopub.status.idle":"2024-05-27T20:07:41.128414Z","shell.execute_reply.started":"2024-05-27T20:07:41.110951Z","shell.execute_reply":"2024-05-27T20:07:41.126925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from flask import Flask, send_file\n\napp = Flask(__name__)\n\n@app.route('/download')\ndef download_file():\n    filename = \"submission.csv\"\n    return send_file(filename, as_attachment=True)\n\nif __name__ == '__main__':\n    app.run()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T20:10:29.089056Z","iopub.execute_input":"2024-05-27T20:10:29.089540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Same as above but with feature importance code\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\nX = traindf[[\"patient_age\", \"payer_type_encoded\", \"health_uninsured\",\"Average of Jul-18\"]]\ny = traindf['metastatic_diagnosis_period']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Initialize the GradientBoostingRegressor with desired parameters\ngb_regressor = GradientBoostingRegressor()\n\n# Train the model\ngb_regressor.fit(X_train, y_train)\n\n# Make predictions on the test data\npredictions = gb_regressor.predict(X_test)\n\n# Calculate Root Mean Squared Error\nrmse = np.sqrt(mean_squared_error(y_test, predictions))\n\nprint(\"Root Mean Squared Error:\", rmse)\n\n# Extract feature importance\nfeature_importance = gb_regressor.feature_importances_\n\n# Sort feature importance in descending order\nsorted_indices = np.argsort(feature_importance)[::-1]\n\n\n# Print feature importance\nfor i in sorted_indices:\n    print(f\"Feature {X.columns[i]}: {feature_importance[i]}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:44:48.198990Z","iopub.execute_input":"2024-05-28T15:44:48.199374Z","iopub.status.idle":"2024-05-28T15:44:48.931801Z","shell.execute_reply.started":"2024-05-28T15:44:48.199344Z","shell.execute_reply":"2024-05-28T15:44:48.930478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#same as above but with more predictors\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\nX = traindf[[\"patient_age\", \"patient_race_encoded\", \"payer_type_encoded\", \"limited_english\", \"health_uninsured\", \"Average of Jan-13\", \"Average of Jul-13\", \"Average of Jan-16\", \"Average of Jul-16\", \"Average of Jan-18\", \"Average of Jul-18\"]] \ny = traindf['metastatic_diagnosis_period']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Initialize the GradientBoostingRegressor with desired parameters\ngb_regressor = GradientBoostingRegressor()\n\n# Train the model\ngb_regressor.fit(X_train, y_train)\n\n# Make predictions on the test data\npredictions = gb_regressor.predict(X_test)\n\n# Calculate Root Mean Squared Error\nrmse = np.sqrt(mean_squared_error(y_test, predictions))\n\nprint(\"Root Mean Squared Error:\", rmse)\n\n# Extract feature importance\nfeature_importance = gb_regressor.feature_importances_\n\n# Sort feature importance in descending order\nsorted_indices = np.argsort(feature_importance)[::-1]\n\n\n# Print feature importance\nfor i in sorted_indices:\n    print(f\"Feature {X.columns[i]}: {feature_importance[i]}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T05:54:38.904943Z","iopub.execute_input":"2024-05-28T05:54:38.906110Z","iopub.status.idle":"2024-05-28T05:54:40.459278Z","shell.execute_reply.started":"2024-05-28T05:54:38.906053Z","shell.execute_reply":"2024-05-28T05:54:40.457665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport numpy as np\n\n# Assuming 'traindf' is your DataFrame containing the data\n# Define your features (X) and target variable (y)\nX = traindf.drop(columns=['metastatic_diagnosis_period'])  # Features\ny = traindf['metastatic_diagnosis_period']  # Target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the Random Forest Regressor with desired parameters\nrf_regressor = RandomForestRegressor()\n\n# Train the model\nrf_regressor.fit(X_train, y_train)\n\n# Make predictions on the test data\npredictions = rf_regressor.predict(X_test)\n\n# Calculate Root Mean Squared Error\nrmse = np.sqrt(mean_squared_error(y_test, predictions))\nprint(\"Root Mean Squared Error:\", rmse)\n\n# Extract feature importance\nfeature_importance = rf_regressor.feature_importances_\n\n# Sort feature importance in descending order\nsorted_indices = np.argsort(feature_importance)[::-1]\n\n# Print feature importance\nfor i in sorted_indices:\n    print(f\"Feature '{X.columns[i]}': {feature_importance[i]}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T05:56:06.057161Z","iopub.execute_input":"2024-05-28T05:56:06.057660Z","iopub.status.idle":"2024-05-28T05:56:57.993222Z","shell.execute_reply.started":"2024-05-28T05:56:06.057618Z","shell.execute_reply":"2024-05-28T05:56:57.991783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport numpy as np\n\n# Assuming 'traindf' is your DataFrame containing the data\n# Define your features (X) and target variable (y)\nX = traindf[[\"patient_age\", \"patient_race_encoded\", \"payer_type_encoded\", \"limited_english\", \"health_uninsured\", \"Average of Jan-13\", \"Average of Jul-13\", \"Average of Jan-16\", \"Average of Jul-16\", \"Average of Jan-18\", \"Average of Jul-18\"]] \ny = traindf['metastatic_diagnosis_period']  # Target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the Random Forest Regressor with desired parameters\nrf_regressor = RandomForestRegressor()\n\n# Train the model\nrf_regressor.fit(X_train, y_train)\n\n# Make predictions on the test data\npredictions = rf_regressor.predict(X_test)\n\n# Calculate Root Mean Squared Error\nrmse = np.sqrt(mean_squared_error(y_test, predictions))\nprint(\"Root Mean Squared Error:\", rmse)\n\n# Extract feature importance\nfeature_importance = rf_regressor.feature_importances_\n\n# Sort feature importance in descending order\nsorted_indices = np.argsort(feature_importance)[::-1]\n\n# Print feature importance\nfor i in sorted_indices:\n    print(f\"Feature '{X.columns[i]}': {feature_importance[i]}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T05:58:51.008685Z","iopub.execute_input":"2024-05-28T05:58:51.009204Z","iopub.status.idle":"2024-05-28T05:58:58.722872Z","shell.execute_reply.started":"2024-05-28T05:58:51.009165Z","shell.execute_reply":"2024-05-28T05:58:58.721595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# sklearn imports\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RandomizedSearchCV, GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor, VotingClassifier, StackingRegressor, GradientBoostingRegressor,  ExtraTreesRegressor\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.feature_selection import RFE, SequentialFeatureSelector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import RidgeCV, LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\n\n# catboost import\nfrom catboost import CatBoostRegressor\n\n# xgboost imports\nfrom xgboost import XGBRegressor, XGBRFRegressor\nimport xgboost as xgb\n\n# lightgbm import\nfrom lightgbm import LGBMRegressor\n\n# Set theme for seaborn\nsns.set_theme(style='white')\n\n# Define RMSE\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\n# Pandas display settings\npd.set_option('display.max_columns', None)\npd.options.display.max_rows = None\npd.set_option('display.max_colwidth', 200)\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set seed for reproducibility\nseed = 42\nplt.style.use('ggplot')\n\n# CatBoost\nprint(\"------------------CatBoost---------------\")\nctb = CatBoostRegressor(\n    random_state=seed,\n    learning_rate=.02,\n    verbose=False,\n    eval_metric='RMSE'\n).fit(X_train, y_train, eval_set=(X_test, y_test), use_best_model=True)\nprint(\"CatBoost RMSE:\", rmse(y_test, ctb.predict(X_test)))\n\n# XGBoost\nprint(\"------------------XGBoost---------------\")\nxgb = XGBRegressor(\n    random_state=seed,\n    learning_rate=.03,\n    max_depth=4\n).fit(X_train, y_train)\nprint(\"XGBoost RMSE:\", rmse(y_test, xgb.predict(X_test)))\n\n# LightGBM\nprint(\"------------------LightGBM---------------\")\nlgbm = LGBMRegressor(\n    random_state=seed,\n    learning_rate=.04,\n    verbose=0\n).fit(X_train, y_train)\nprint(\"LightGBM RMSE:\", rmse(y_test, lgbm.predict(X_test)))\n\n# Random Forest\nprint(\"------------------Random Forest---------------\")\nrf = RandomForestRegressor(\n    random_state=seed,\n    max_depth=6\n).fit(X_train, y_train)\nprint(\"Random Forest RMSE:\", rmse(y_test, rf.predict(X_test)))\n\n# Gradient Boosting\nprint(\"------------------Gradient Boosting---------------\")\ngb = GradientBoostingRegressor(\n    random_state=seed,\n    learning_rate=.05\n).fit(X_train, y_train)\nprint(\"Gradient Boosting RMSE:\", rmse(y_test, gb.predict(X_test)))\n\n# Extra Trees\nprint(\"------------------Extra Trees---------------\")\net = ExtraTreesRegressor(\n    random_state=seed,\n    max_depth=9\n).fit(X_train, y_train)\nprint(\"Extra Trees RMSE:\", rmse(y_test, et.predict(X_test)))","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:59:12.600412Z","iopub.execute_input":"2024-05-28T15:59:12.600844Z","iopub.status.idle":"2024-05-28T15:59:58.751682Z","shell.execute_reply.started":"2024-05-28T15:59:12.600809Z","shell.execute_reply":"2024-05-28T15:59:58.750263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport numpy as np\n\n\nX = traindf[[\"patient_age\", \"patient_race_encoded\", \"payer_type_encoded\", \"limited_english\", \"health_uninsured\", \"Average of Jan-13\", \"Average of Jul-13\", \"Average of Jan-16\", \"Average of Jul-16\", \"Average of Jan-18\", \"Average of Jul-18\"]] \ny = traindf['metastatic_diagnosis_period']  # Target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the Random Forest Regressor with desired parameters\nrf_regressor = RandomForestRegressor()\n\n# Train the model\nrf_regressor.fit(X_train, y_train)\n\n# Make predictions on the test data\npredictions = rf_regressor.predict(X_test)\n\n# Calculate Root Mean Squared Error\nrmse = np.sqrt(mean_squared_error(y_test, predictions))\nprint(\"Root Mean Squared Error:\", rmse)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T05:42:23.798733Z","iopub.execute_input":"2024-05-29T05:42:23.799180Z","iopub.status.idle":"2024-05-29T05:42:31.406018Z","shell.execute_reply.started":"2024-05-29T05:42:23.799140Z","shell.execute_reply":"2024-05-29T05:42:31.404737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#START HERE FOR TEST\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor  \nimport pandas as pd\nimport numpy as np\n\npatient_ids=testdf[\"patient_id\"]\nX = traindf[[\"patient_age\", \"patient_race_encoded\", \"payer_type_encoded\", \"limited_english\", \"health_uninsured\", \"Average of Jan-13\", \"Average of Jul-13\", \"Average of Jan-16\", \"Average of Jul-16\", \"Average of Jan-18\", \"Average of Jul-18\"]] \ny = traindf['metastatic_diagnosis_period']\n\nX_test = testdf[[\"patient_age\", \"patient_race_encoded\", \"payer_type_encoded\", \"limited_english\", \"health_uninsured\", \"Average of Jan-13\", \"Average of Jul-13\", \"Average of Jan-16\", \"Average of Jul-16\", \"Average of Jan-18\", \"Average of Jul-18\"]] \n\nmodel = RandomForestRegressor() \n\nmodel.fit(X_train, y_train)\n\npredictions=model.predict(X_test)\n\n# Create a DataFrame to store the results, including patient ID\nresults_df = pd.DataFrame({\"patient_id\": patient_ids, \"predicted_value\": predictions})\n\n# Print or output results table\nprint(results_df.head())\n\n# Save results table to CSV file\nresults_df.to_csv('predictions_with_patient_id.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T06:11:15.653850Z","iopub.execute_input":"2024-05-29T06:11:15.654279Z","iopub.status.idle":"2024-05-29T06:11:23.299414Z","shell.execute_reply.started":"2024-05-29T06:11:15.654247Z","shell.execute_reply":"2024-05-29T06:11:23.297922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df.to_csv('predictions_with_patient_id.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T06:12:18.203920Z","iopub.execute_input":"2024-05-29T06:12:18.204413Z","iopub.status.idle":"2024-05-29T06:12:18.229825Z","shell.execute_reply.started":"2024-05-29T06:12:18.204374Z","shell.execute_reply":"2024-05-29T06:12:18.228548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract patient IDs\npatient_id_dict = testdf[\"patient_id\"]\n\n# Make predictions on the test data\ntest_predictions = model.predict(testdf.drop(\"patient_id\", axis=1)).round().astype(int)\n\n# Create a DataFrame to store the results\nresults_df = pd.DataFrame({'patient_id': patient_id_dict, 'metastatic_diagnosis_period': test_predictions})\ndisplay(results_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-05-29T06:07:40.115637Z","iopub.execute_input":"2024-05-29T06:07:40.116024Z","iopub.status.idle":"2024-05-29T06:07:40.225166Z","shell.execute_reply.started":"2024-05-29T06:07:40.115993Z","shell.execute_reply":"2024-05-29T06:07:40.223543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the Random Forest Regressor with desired parameters\nrf_regressor = RandomForestRegressor()\n\n# Train the model\nrf_regressor.fit(X_train, y_train)\n\n# Make predictions on the test data\npredictions = rf_regressor.predict(X_test)\n\n# Calculate Root Mean Squared Error\nrmse = np.sqrt(mean_squared_error(y_test, predictions))\nprint(\"Root Mean Squared Error:\", rmse)\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\nX = testdf[[\"patient_age\", \"patient_race_encoded\", \"payer_type_encoded\", \"limited_english\", \"health_uninsured\", \"Average of Jan-13\", \"Average of Jul-13\", \"Average of Jan-16\", \"Average of Jul-16\", \"Average of Jan-18\", \"Average of Jul-18\"]] \ny_pred = model.predict(X_test)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Initialize the GradientBoostingRegressor with desired parameters\ngb_regressor = GradientBoostingRegressor()\n\n# Train the model\ngb_regressor.fit(X_train, y_train)\n\n# Make predictions on the test data\npredictions = gb_regressor.predict(X_test)\n\n# Calculate Root Mean Squared Error\nrmse = np.sqrt(mean_squared_error(y_test, predictions))\n\nprint(\"Root Mean Squared Error:\", rmse)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport numpy as np\n\n# Assuming 'traindf' is your DataFrame containing the data\n# Define your features (X) and target variable (y)\nX = traindf.drop(columns=['metastatic_diagnosis_period'])  # Features\ny = traindf['metastatic_diagnosis_period']  # Target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the Random Forest Regressor\nrf_regressor = RandomForestRegressor()\n\n# Define the grid of hyperparameters to search\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Initialize GridSearchCV\ngrid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n\n# Perform grid search\ngrid_search.fit(X_train, y_train)\n\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n\n# Get the best model\nbest_rf_model = grid_search.best_estimator_\n\n# Make predictions on the test data using the best model\npredictions = best_rf_model.predict(X_test)\n\n# Calculate Root Mean Squared Error\nrmse = np.sqrt(mean_squared_error(y_test, predictions))\nprint(\"Root Mean Squared Error:\", rmse)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:45:43.968886Z","iopub.execute_input":"2024-05-28T15:45:43.969468Z","iopub.status.idle":"2024-05-28T15:58:09.947220Z","shell.execute_reply.started":"2024-05-28T15:45:43.969377Z","shell.execute_reply":"2024-05-28T15:58:09.945159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract feature importance\nfeature_importance = gb_regressor.feature_importances_\n\n# Sort feature importance in descending order\nsorted_indices = np.argsort(feature_importance)[::-1]\n\n\n# Print feature importance\nfor i in sorted_indices:\n    print(f\"Feature {i}: {feature_importance[i]}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T05:39:54.263083Z","iopub.execute_input":"2024-05-28T05:39:54.263547Z","iopub.status.idle":"2024-05-28T05:39:54.273833Z","shell.execute_reply.started":"2024-05-28T05:39:54.263514Z","shell.execute_reply":"2024-05-28T05:39:54.272502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX = traindf.drop(columns=[\"patient_age\", \"patient_race_encoded\", \"payer_type_encoded\", \"limited_english\", \"health_uninsured\"])\ny = traindf ['metastatic_diagnosis_period']\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create Gradient Boosting Classifier model\nmodel = GradientBoostingClassifier()\n\n# Fit the model to the training data\nmodel.fit(X_train, y_train)\n\n# Predict class probabilities on the test data\ny_prob = model.predict_proba(X_test)\n\n# Convert class probabilities to continuous predictions\n# For simplicity, you can take the probability of the positive class\ny_pred_continuous = y_prob[:, 1]\n\n# Calculate RMSE\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_continuous))\nprint(\"RMSE:\", rmse)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:48:13.097798Z","iopub.execute_input":"2024-05-27T17:48:13.099162Z","iopub.status.idle":"2024-05-27T19:11:19.996414Z","shell.execute_reply.started":"2024-05-27T17:48:13.099115Z","shell.execute_reply":"2024-05-27T19:11:19.995151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting  # Enable HistGradientBoosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX = pd.DataFrame(traindf, columns=[\"patient_age\", \"patient_race_encoded\", \"payer_type_encoded\", \"limited_english\", \"health_uninsured\"])\ny = pd.DataFrame(traindf['metastatic_diagnosis_period'], columns=['metastatic_diagnosis_period'])\n\n# Assuming 'X' is your feature matrix and 'y' is your target variable for regression\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Train HistGradientBoostingClassifier model\nclf = GradientBoostingClassifier()\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = clf.predict(X_test)\n\n# Evaluate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\nfrom sklearn.metrics import brier_score_loss\nimport numpy as np\n# Assuming 'clf' is your trained Gradient Boosting Classifier and 'X_test' and 'y_test' are your test data\ny_prob = clf.predict_proba(X_test)[:, 1]  # Probability of positive class\nbrier_score = brier_score_loss(y_test, y_prob)\nrmse_like_metric = np.sqrt(brier_score)\n\nprint(\"RMSE-like Metric:\", rmse_like_metric)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = rf_model.predict(X_train)\n\n# Calculate the mean squared error (MSE) with all variables\nmse_all_vars = mean_squared_error(y_train, y_pred)\n\n# Initialize a dictionary to store RCE for each variable\nrce_dict = {}\n\n# Iterate over each feature\nfor feature in X_train.columns:\n    # Create a copy of X_train without the current feature\n    X_temp = X_train.drop(columns=[feature])\n       # Train the Random Forest model without the current feature\n    rf_model_temp = RandomForestRegressor(n_estimators=100, random_state=42)\n    rf_model_temp.fit(X_temp, y_train.values.ravel())\n    \n    # Predict on the test set\n    y_pred_temp = rf_model_temp.predict(X_temp)\n    \n    # Calculate the mean squared error (MSE) without the current feature\n    mse_temp = mean_squared_error(y_train, y_pred_temp)\n    \n    # Calculate the Relative Change in Error (RCE) for the current feature\n    rce = (mse_temp - mse_all_vars) / mse_all_vars\n    \n    # Store the RCE in the dictionary\n    rce_dict[feature] = rce","metadata":{},"execution_count":null,"outputs":[]}]}